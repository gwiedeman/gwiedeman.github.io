<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-11-13T23:27:48-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Greg Wiedeman</title><subtitle>I'm an archivist who works to make information more accessible.</subtitle><author><name>Gregory Wiedeman</name></author><entry><title type="html">Migrating to ArchivesSpace at UAlbany</title><link href="http://localhost:4000/2017/04/01/migrating-to-archivesspace.html" rel="alternate" type="text/html" title="Migrating to ArchivesSpace at UAlbany" /><published>2017-04-01T00:00:00-04:00</published><updated>2017-04-01T00:00:00-04:00</updated><id>http://localhost:4000/2017/04/01/migrating-to-archivesspace</id><content type="html" xml:base="http://localhost:4000/2017/04/01/migrating-to-archivesspace.html">&lt;p&gt;This is an overview our ArchivesSpace migration process and descriptions of the code used. Hopefully this rather long-winded description helps document our processes and thinking.&lt;/p&gt;

&lt;p&gt;Overall, my strategy was to import everything twice. Once we got most of our data into ASpace, I let our staff have a free run of the data to get comfortable with the application. We maintained our previous data stores as masters, and staff could create, edit, and delete ASpace data as much as they wanted. When we were done testing and I was comfortable with all the migration scripts, we cleared out all the data and made a clean import.&lt;/p&gt;

&lt;h2 id=&quot;clean-up-ead-files-for-the-archivesspace-importer&quot;&gt;Clean-up EAD files for the ArchivesSpace importer&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;importFix.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step one was getting all of our EAD data reformatted so they would all easily import into ASpace. I wrote &lt;code class=&quot;highlighter-rouge&quot;&gt;importFix.py&lt;/code&gt; to make duplicate copies of all of our EAD XML files while stripping out and cleaning any issues that I found would cause errors during import. An example issue was that ASpace didn’t want extent units stored in an attribute like 
  &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;extent unit=&quot;cubic ft.&quot;&amp;gt;21&amp;lt;/extent&amp;gt;&lt;/code&gt; Instead, it wanted &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;extent&amp;gt;21 cubic ft.&amp;lt;/extent&amp;gt;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We had been using an internal semantic ID system, but we wanted to get rid of these and rely on the ASpace ref_IDs instead, so the script also removes all the @id attributes in &lt;c01&gt;, &lt;c02&gt;, etc.&lt;/c02&gt;&lt;/c01&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can also fix these issues with a plugin that customized how ASpace imports the files. Alex Duryee has spoken on this at &lt;a href=&quot;https://github.com/alexduryee/saa-aspace-demo&quot;&gt;SAA&lt;/a&gt; and &lt;a href=&quot;https://github.com/alexduryee/beyondthebasics&quot;&gt;elsewhere&lt;/a&gt;. I was already much more comfortable with Python and lxml, so this was faster for a one-off import process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While we had employed a lot of rule-based validation to ensure standardization. There were other issues, like ASpace understood that 1988-03-30/1981-01-01 was an invalid date, but our validation script did not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the script also identified components with empty &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;unittitle&amp;gt;&lt;/code&gt; tags, etc., so I could get a list of these issues prior to import. Once I had identified the most common problems, this was much easier than having background jobs keep failing halfway through.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This was a tedious process with lots of trial and error that took almost 2 complete days. I had to re-import a bunch of times, so I wrote &lt;code class=&quot;highlighter-rouge&quot;&gt;deleteRepoRecords.py&lt;/code&gt; to delete all resource records in ASpace (obviously use cautiously).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The feedback from ASpace on import issues can be really frustrating. It does not give you feedback on the EAD file that was the culprit, but points you to the new native ASpace record that was never actually created. You have to dig a bit in the log to find the EAD filename. Since I has stripped out our IDs, I was only left with the ASpace ref_id which is not helpful at all in finding individual components among our EAD jungle, so I had to rely on the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;unittitle&amp;gt;&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;unitdate&amp;gt;&lt;/code&gt; strings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You’ll see feedback like, “there’s an issue in note[7].” It takes a while to figure out that note[7] means &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;physdesc&amp;gt;&lt;/code&gt;, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We only have one real import incompatibility with  ASpace, and that was only one EAD file where 7,800 lines of the 13,000 line file was one gigantic &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;chronlist&amp;gt;&lt;/code&gt;. ASpace rightly denied this garbage, so we just pulled it out of the finding aid and maybe it will become a part of a web exhibit some day.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We still had a some messy data when you checked the controlled values lists, but these were few enough that it was easier to identify and clean these issues after the migration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the end of this process, &lt;code class=&quot;highlighter-rouge&quot;&gt;importFix.py&lt;/code&gt; allowed us to keep creating and maintaining our EAD files in our current systems, but also let me quickly create ASpace-friendly versions when I need to.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dont-repeat-yourself&quot;&gt;Don’t Repeat Yourself&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/UAlbanyArchives/archives_tools&quot;&gt;aspace Python Library&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When I wrote the delete resources script I discovered that working with the API is awesome, but I was also repeatedly calling the same API requests. There is no complete Python library for the ArchivesSpace API, so most people make calls manually with &lt;a href=&quot;http://docs.python-requests.org/en/master/&quot;&gt;requests&lt;/a&gt;. Artefactual labs also has a &lt;a href=&quot;https://github.com/artefactual-labs/agentarchives&quot;&gt;basic library&lt;/a&gt; for this, but it doesn’t do a lot for you and still involves manually dealing with a lot of JSON in a way that I didn’t find intuitive. Basically, I wanted to iterate though the ASpace data as easily as lxml loops though XML data. This often involves multiple API calls for a single function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the course of the migration I started writing a bunch of functions that manage the API behind the scenes. This was I can quickly iterate though collections with &lt;code class=&quot;highlighter-rouge&quot;&gt;getResources()&lt;/code&gt;, collection arrangement with &lt;code class=&quot;highlighter-rouge&quot;&gt;getTree()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;getChildren()&lt;/code&gt; and even find a collection by its identifier with &lt;code class=&quot;highlighter-rouge&quot;&gt;getResourceID()&lt;/code&gt;, or a location by its title string with &lt;code class=&quot;highlighter-rouge&quot;&gt;findLocations()&lt;/code&gt;. Its also handy to have some blank objects at your fingertips, like with &lt;code class=&quot;highlighter-rouge&quot;&gt;makeArchObj()&lt;/code&gt;, so you can create new archival objects without knowing exactly what JSON ASpace needs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There’s also some debugging tools I use all time. &lt;code class=&quot;highlighter-rouge&quot;&gt;pp()&lt;/code&gt; pretty prints an object to the console as JSON, &lt;code class=&quot;highlighter-rouge&quot;&gt;fields()&lt;/code&gt; lists all the keys in an object, and &lt;code class=&quot;highlighter-rouge&quot;&gt;serializeOutput()&lt;/code&gt; writes objects to your filesystem as .json files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The library uses &lt;a href=&quot;https://pypi.python.org/pypi/easydict/&quot;&gt;easydict&lt;/a&gt; to make objects out of the JSON, so you can call it with dot notation (&lt;code class=&quot;highlighter-rouge&quot;&gt;print collection.title&lt;/code&gt;). Writing this library enabled me from rewriting the same code, or finding that script I used last week to steal some lines from. Its really cool to be able to do this:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  from archives_tools import aspace as AS
    
  session = AS.getSession()
  repo = &quot;2&quot;
    
  for collection in AS.getResources(session, repo, &quot;all&quot;):
  	if collection.title = &quot;David Baldus Papers&quot;:
  		collection = AS.makeDate(collection, 1964-11, 2001-03)
  		post = AS.postResource(session, repo, collection)
  		print (post)
    		
  &amp;gt; 200
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some caveats here. There are a bunch of decisions you make while writing these that I kind of made arbitrarily. It’s probably more complex than it has to be. Ideally, this would be more a a communally-driven effort based on a wider set of needs and perspectives. The code is completely open, so if nothing else, I guess this could provide an example of what a Python library for ASpace might look like. I’m going to try and complete the documentation soon, and if anyone has suggestions I’d really like to hear them. If the community moves to a different approach, I’ll work to update all of our processes to match the consensus, since it will be better than what I can come up with on my own.&lt;/p&gt;

&lt;h2 id=&quot;import-collection-level-records&quot;&gt;Import collection-level records&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateCollections.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Before ArchivesSpace, we used a custom “CMS” system for collection management and accessioning. We think it’s over 15 years old, and the only way to get data out is directly from the database tables which looked like they were copied from some other system and were overtly complicated and often repeated data. Moreover, the only way to view entire records was to go into edit mode where you could easily overwrite data. If you picked an accession number for a new accession that was already taken, it would just override the old record (!!!). Overall it was a pain to enter information in the system and over time the department slowly stopped updating all but the most necessary fields. Yet, this system had the only total listing of all collections.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Over the past year, we found that as we got more and more of our archival description into EAD, we needed an automated way to push this data into our Drupal-driven website, so I pulled a master collection list out of this collection management system, updated and fixed it and just dumped it into a spreadsheet. For each collection it had a normalized name, listed if there was an EAD or HTML finding aid, and had an extent, inclusive date, and abstract for everything not in EAD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While we’ve been making great strides in getting collection-level records for everything, almost 1/3 on our collections sill only had accession records in the old system, and a row in this spreadsheet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This spreadsheet has been our master collection list and a key part of the backend of &lt;a href=&quot;http://library.albany.edu/archive&quot;&gt;our public access system&lt;/a&gt; (not recommended). ArchivesSpace now allows us to replace this setup by pulling all of this information directly from the API.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yet, with this spreadsheet I could use &lt;a href=&quot;https://openpyxl.readthedocs.io/en/default/&quot;&gt;a Python library for .xslx files&lt;/a&gt; to import some basic resource records into ASpace. While they wouldn’t be DACS-compliant, we could still export this basic data to our public access system, and link them to accession records we’ll import later.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;migrateCollections.py&lt;/code&gt; iterates though the spreadsheet and uses the aspace library to post new resource records in ASpace.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;migrating-accessions&quot;&gt;Migrating Accessions&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateAccessions.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For accessions I asked the Library’s awesome database person Krishna for help. Our old “CMS” was a .asp web application which, from what I can tell, used an old SQL server for its database. I needed something that I could loop though with Python so I asked him to export some CSVs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once he made the first export we noticed some problems. First there were a lot of commas in some of the fields, so we had to use a different delimiter. The other problem was there was a bunch of carriage returns that use the same character as the csv new line, so we had to escape all these prior to the export. I tried fixing it with a Python script by counting the number of fields in each row and piecing it back together humpty-dumpty-style, but I never want to do something that frustrating again. Luckily with a little time, Krishna was able to escape them before export and use pipes (&lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt;) as the delimiter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once I was able to loop through the table with all the accessions, I met with Jodi and Melissa, some of our other archivists, to identify which fields were still held useful data and how to map them to ASpace accession records. Honestly, we threw out half the data because it was useless, but we were able to reclaim all the accession dates, descriptions, and extents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We had a handful of issues that had to be edited manually. Even though the system returned an error if you forgot to enter an accession number, there were a bunch of records without numbers or records without numbers. &lt;code class=&quot;highlighter-rouge&quot;&gt;checkAccessions.py&lt;/code&gt; identified these issues, but we had to go look at each record and try to parse out what happened. The good thing was the data did include a last updated timestamp, which helped a lot.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Of course the first time I tried this I opened the CSV file in Excel it unhelpfully decided to reformat all of the timestamps and drop all the trailing zeros. Good thing I kept a backup file. Since it was only a handful I ended up doing the edits in a text editor, just to be safe.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The cool thing was that the collection IDs were really standardized, so we could easily match them up with the resource records we just created. We just had to convert APAP-312 to apap312, which is a piece of cake with Python’s string methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The aspace library really came in handy here. Two functions, &lt;code class=&quot;highlighter-rouge&quot;&gt;makeExtent()&lt;/code&gt; and  &lt;code class=&quot;highlighter-rouge&quot;&gt;makeDate()&lt;/code&gt; just add extents and dates to any ASpace JSON object. This worked with accessions just as it did with resources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The extent was just an uncontrolled string field. Considering this, we had used the field very consistently, but of course there were a bunch of typos or oddly worded unit descriptions. I cleaned up the vast bulk of these issues with Python’s string methods, but there were still some “long-tail” issues that didn’t seem to be worth spending hours on. After all, we haven’t needed real machine-readable accession extents yet, and its not publicly-visible data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For now I just allowed a new extent type “uncontrolled,” and threw the whole field in the number field. We can just fix these one by one as we come across them. I guess I could have dumped the CSV into OpenRefine, but I can also come back later and export a new CSV with the API if we need to clean these up further.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After running &lt;code class=&quot;highlighter-rouge&quot;&gt;migrateAccessions.py&lt;/code&gt;, we had accession records and collection descriptions in the same place for the first time. I was really wonderful seeing those Related Accessions and Related Resources links pop up in ASpace.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;locations&quot;&gt;Locations&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	migrateLocations.py
	exportContainers.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Locations turned out to be the trickiest part of the migration, because our local practices really conflicted with how ASpace manages locations. Theory-wise ASpace does this much better and eventually our practices will improve after the migration, but I had to do some hacky stuff to get ASpace to work with our locations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Basically, in ASpace 1.5+ containers are separated out and managed separately from archival description and connected to both location and description records with URI link. This is great because it lets us describe records independently from how they are managed physically. The newer ASpace versions also let you do some really cool things like calculate extents if you have good data and also supports barcoding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In ASpace collections don’t have locations. How can they, since archival description is really an intangible layer of abstraction that just refers to boxes and folders? In ASpace, collections (or any other description records) are linked to Top Containers (boxes) which are linked to locations. Long-term this is great, since it lets us manage the intellectual content of a collection differently from its physical content.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yet, this poses a very practical problem for us. Our old “CMS” had only one location text field per collection, so we really only have collection-level locations. We’ve never really had a problem with our location records because we have really awesome on-site storage. It also helps that most of our staff has been here a long time, and at least generally knows where most collections are anyway.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The location field we had was really messy. This is one extreme real-world example:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  H-16-3-1/H-16-2-3,(last shelf has microfilm and CDs with images used for LUNA); 
  G-8-1-1/G-8-2-1; G-17-3-2/G-17-3-1; G-17-4-1/G-17-4-7; G-10-4-1/G-10-4-3;
  G-10-5-1/G-10-5-8;C-12-2-1/C-12-2-5; C-12-1-2/-12-1-3; SB 17 - o-15 (bound
  copies of Civil Service Leader digitized by Hudson Micro and returned in 
  2013, bound and unbound copies of The Public Sector, 1978-1998, digitized
  and returned in 2015, unbound copies of The Work Force, 1998 through 2012, 
  also digitized); Cold 1-1 - 1-4, E-1-1, A-1-7 - A-1-8, A-1-5 - A-1-7,
  A-1-9, A-1-8, A-1-8 - A-1-9, A-5-1 - A-5-2, A-5-2 - A-5-3, A-4-9 - A-5-1,
  A-6-5, A-5-5, A-6-4 - A-6-5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So that collection has a bunch of top containers, how do I assign that mess to them? Well this collection in particular is currently being, well, &lt;em&gt;reworked&lt;/em&gt;, but for everything else we had to standardize the data into one format, basically:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  G-4-2-3/G-4-2-7 (whatever note here); J-3-3-5 (second location here); I-7-3-6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then one of our awesome graduate students, Ben Covell, learned &lt;a href=&quot;http://openrefine.org/&quot;&gt;OpenRefine&lt;/a&gt; in about a hour and cleaned up all the data in less than a day. (Part of the solution always seems to be having great grad students)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, we decided it wasn’t worth it to go through every box in the archives to get individual shelf coordinates. Thus, we had two options, either use a separate system for shelf lists, or hack ASpace to make it work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I tried and failed to get ASpace to accept multiple locations per box, which seems like it really shouldn’t be allowed. You can actually have a top container linked to multiple shelves, but it won’t let you then link those boxes to description records.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASpace does let you have previous locations though, but it requires an end date, and the location record to have a temporary label. We decided to have a temporary “collection_level” label and setting the end date to 2999-01-01, and hope that we weren’t creating an insurmountable Y3K problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Doing it this way meant a lot of work with the API, because we had to find every top container assigned to each collection, translate &lt;code class=&quot;highlighter-rouge&quot;&gt;G-17-3-8&lt;/code&gt; to a ASpace location record, find that record, update that location record to be temporary, and update the top_container record to add each location. This is exactly what &lt;code class=&quot;highlighter-rouge&quot;&gt;migrateLocations.py&lt;/code&gt; does, and with the library it only takes 200 lines of code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So that means if a collection-level location was &lt;code class=&quot;highlighter-rouge&quot;&gt;C-24-2-1/C-24-3-8&lt;/code&gt; (16 shelves), every box in that collection (probably about 40-50 boxes) would be linked to each of those 16 locations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For larger collections, this would be a bit unwieldy, so the plan is to list individual shelves for larger collections now, and do the rest over time. &lt;code class=&quot;highlighter-rouge&quot;&gt;exportContainers.py&lt;/code&gt; helps with this. It makes a .xlsx spreadsheet for each collection that lists every top container and its URI. So we have a directory of spreadsheet files for each collection like apap101.xlsx. We can list the shelf coordinates for each box in that spreadsheet in our same format (&lt;code class=&quot;highlighter-rouge&quot;&gt;K-2-2-3&lt;/code&gt;) and use the API to update each top container record over time. Now whenever we’re working with a collection we can fix this problem one by one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, ASpace comes with some really cool features like container profiles and shelf profiles, but they’re not mandatory. We decided it wasn’t worth it yet to measure each shelf, but we hope to do this over time in the next couple of years.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, migrating locations “the right way” would have taken months and months of effort and involved every staff member in the department. Instead, I was able to basically do it myself with a little help in about a week. We might run into a problem if updates to ASpace changes how the application handles these records, but even in that worst-case scenario we can just export all our incompatible locations with the API before upgrading and use a separate shelf list system. Our data will even be a lot better too.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learn-by-breaking-things&quot;&gt;Learn by Breaking Things&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We really didn’t have the option to bring in Lyrasis to do workshops for our staff on how to use ASpace. This is going to be the primary medium for our department to create metadata and make our holdings available, so they need to be comfortable with it, and switching everyone over to a whole new system without any formal prep is a lot to ask.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best way I find to learn a tool is to get some hands-on experience and try and break it to see where the boundaries are. Migrating all our data twice, not only lowered the stakes for the first import, but it gave us a lot of time with a fully-running ArchivesSpace with a lot of realistic test data. I opened it up to the our department and encouraged them to make new resources and accessions, delete things, and generally try test their boundaries and get comfortable. I pointed them to &lt;a href=&quot;http://www2.archivists.org/sites/all/files/OK%20State%20U%20Using%20ArchivesSpace.pdf&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;http://www2.archivists.org/sites/all/files/UNO_ArchivesSpace_Walkthroughs_2015.pdf&quot;&gt;guides&lt;/a&gt; shared by the SAA Collection Management Tools Section, and they also said there’s a bunch of &lt;a href=&quot;https://www.youtube.com/results?search_query=archviesspace&quot;&gt;screencasts on YouTube&lt;/a&gt; that really helped.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, ASpace is very intuitive, and easy to use if you comfortable working with web applications. Our department has taken to it fairly quickly, and I don’t think more formal training would of helped much. Having a really good sandbox and dedicating real time to experimenting with it is definitely key.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We’re also still working on establishing best practices for use and documentation for our students. I think this is actually a bigger hurdle than leaning the tool itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;building-satellite-tools&quot;&gt;Building Satellite Tools&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The best reason for implementing ArchivesSpace isn’t necessarily its basic functionally of creating and managing archival description. ASpace’s open architecture means that essentially all the data in the system can be read or updated with the API. This is not only for automating changes, but it also allows us to use other tools to interact with our data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The other really cool thing is that the ASpace API is backed by a data model. This way it closely defines its acceptable data, and rejects anything outside of its parameters. I’ve found that this is makes life so much easier than say, managing your data in XML, since it has the effect of making your data much more stable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If ASpace doesn’t do something the way you like, you can just use a different tool to interact with your ASpace data. This way, ASpace can really support workflows that embrace the separation-of-concerns principle. Here ASpace is at the center of an open archives ecosystem, and you can have smaller, impermanent tools to fill specific functions really well. Often, a modular system like this can be easier to maintain, as in the future you can replace these smaller tools one-by-one as they become obsolete, rather than the daunting task of one huge migration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A lot of this is built upon the work Hillel Arnold and Patrick Galligan have done at the Rockefeller Archive Center. They wrote a &lt;a href=&quot;https://github.com/RockefellerArchiveCenter/as-cors&quot;&gt;plugin&lt;/a&gt; that lets you work with the API from a &lt;a href=&quot;http://blog.rockarch.org/?p=1610&quot;&gt;single HTML page with JavaScript&lt;/a&gt;. They used this to write &lt;a href=&quot;https://github.com/RockefellerArchiveCenter/find-it&quot;&gt;Find-it&lt;/a&gt;, a &lt;a href=&quot;http://blog.rockarch.org/?p=1621&quot;&gt;simple text field to return locations for ASpace ref_ids&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find-It solved a key problem for us as one of the only things our old “CMS” did really well was that you could type in “Crone” and quickly get shelf locations for the Michelle Crone Papers. Our other archivists were asking for something similar in ArchivesSpace, and didn’t like they you had to look through three different records to get this information in ASpace. Yet, the odd way I did our locations posed some problems. Ref_ids would also work, but we needed to return locations for top containers assigned to resources as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With my hacky JavaScript skills (and some help from Slack) I set it up to also return locations for resources using the search API endpoint and the id_0 field. Using the search API made me realize that I could also make an API call that returns a keyword search for resources. The search returns links to the Resource page in ASpace and XTF as well, which makes Find-it a really quick first access point to our collections as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The last request I got was for some way to access locations from resources with lower levels of description, so I was even able to call the resource tree and add links to lower levels if there are no instances. I just made find-it accept ids as a hash and these links just reloads the page with that lower-level ref_id. Overall it became a bit more complex than I was envisioning. Our fork is &lt;a href=&quot;https://github.com/UAlbanyArchives/find-it&quot;&gt;here&lt;/a&gt;, but I’d recommend starting with Rockefeller’s much cleaner version and using ours as more of an example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, the single search bar on a white background was really boring, so I wrote a quick script that updates the background to &lt;a href=&quot;http://www.bing.com/gallery/&quot;&gt;Bing’s image of the day&lt;/a&gt; &lt;a href=&quot;http://stackoverflow.com/questions/10639914/is-there-a-way-to-get-bings-photo-of-the-day&quot;&gt;API&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;findit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Another workflow issue we found was how tedious it could be to enter in individual containers for large accessions. Say we got a new collection of 50 boxes. ASpace makes you create each of these boxes, which is good, but very tedious. So I basically used the same framework as Find-it, figure our how to get ASpace to &lt;a href=&quot;https://github.com/UAlbanyArchives/as-cors&quot;&gt;accept Cross-Origin post requests&lt;/a&gt;, and made a single-page box uploader. Now we can just list new boxes in these fields with our old-style shelf coordinates, and a bunch of JavaScript will create and assign all these new top containers to a resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;boxes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The last request I got from out staff was for creating container list inventories. We use undergraduate student assistants to do most of this data entry work, and we found that ASpace rapid data entry would require a good amount of training for our students who work few hours with a really high turnover rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So I wrote &lt;a href=&quot;https://github.com/UAlbanyArchives/asInventory&quot;&gt;some Python scripts&lt;/a&gt; to manage inventory listings with spreadsheets and the ASpace API. One parses a .xslx file and uploads file-level archival objects through the API. It will also take a bunch of access files and makes digital objects, places the files on our server and links them to archival objects. Another script reads an existing inventory back to a spreadsheet with all the relevant URIs so we can roundtrip these inventories for future updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I also envision this as a near-term solution, as its possible that we end up doing all this natively in ASpace in the future. The cool thing is that the ASpace API makes all these tools really easy and quick to make, and they solve real immediate problems for us. Yet, none of these tools are really essential for us, and if they become too difficult to manage, we can just drop them and move on to something else.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;migration-days&quot;&gt;Migration Day(s)&lt;/h2&gt;

&lt;h3 id=&quot;ead-import&quot;&gt;EAD import&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;importFix.py
checkEAD.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When migration day came, we dumped the database, deleted the index, and started from scratch. I had Krishna export new CSVs from our old system with any updated records that were made since the previous export. By this point the plan was that migration scripts were done and tested and everything would go relatively seamlessly. I was hoping to get everything imported in a day or so, and minimize the downtime when staff were not allowed to make updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When it came time for the EAD import, some recent EADs/updated had happened, and I still got 2 minor errors for about 630 EADs, so I didn’t actually get the Holy Grail of a perfect EAD import. &lt;code class=&quot;highlighter-rouge&quot;&gt;1963-0923&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;1904-01/1903-07&lt;/code&gt; are not valid dates and ArchivesSpace, but these were easy fixes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One thing I had noticed during the first time was than when you import a large amount of EADs, it takes the index awhile to catch up. ASpace still told me I had only 465 resources instead of 633, but that soon updated to 503. Yet, in about an hour I was still missing 29 collections. I assumed that this was still the index, but I quickly wrote checkEAD.py to see how many collections I could access though the API and I was still missing 29.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It turned out that two import jobs had actually failed, but ASpace listed them as Complete for some reason. One issue was when we changed an ID and there was an eadid conflict, and the other issue was that I totally forgot about that huge &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;chronlist&amp;gt;&lt;/code&gt; that failed the first time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These were easy fixes but the lesson here is always test your imports and don’t trust the index.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The importing ended up taking most of the afternoon, so I discovered  my one-day migration plan was over-optimistic.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;clean-resource-identifiers&quot;&gt;Clean Resource Identifiers&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fixNam.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For collection identifiers we had been unsing the prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;nam_&lt;/code&gt; which is our OCLC repository ID or something, but we decided that this was overtly complex and not useful, so we decided to drop these late in the migration planning. While the &lt;code class=&quot;highlighter-rouge&quot;&gt;importFix.py&lt;/code&gt; script was written to clear the &lt;code class=&quot;highlighter-rouge&quot;&gt;nam_&lt;/code&gt; prefix from the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;ead&amp;gt; @id&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eadid&amp;gt;&lt;/code&gt;, but I forgot to do this to the collection-level &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;unitid&amp;gt;&lt;/code&gt;. Unfortunately I found out the hard way that this is what ASpace ends up using as the &lt;code class=&quot;highlighter-rouge&quot;&gt;id_0&lt;/code&gt;, so I whipped up &lt;code class=&quot;highlighter-rouge&quot;&gt;fixNam.py&lt;/code&gt; to strip this post-import rather than start the process over again.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;collection-level-spreadsheet-import&quot;&gt;Collection-level spreadsheet import&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateCollections.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Importing the basic collection-level records from the spreadsheet went fairly well. I realized that we also needed a normalized name for each collection in the resource records somewhere. For example, the “Office of Career and Professional Development Records” needed to be accessible somewhere as “Career and Professional Development, Office of; Records” so it would be alphabetized correctly in our front end. So, I added a few lines in &lt;code class=&quot;highlighter-rouge&quot;&gt;migrateCollections.py&lt;/code&gt; that adds these normal names in the Finding Aid title field.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One annoying thing was that after I posted all the spreadsheet records, the index was a bit slow to update. I was all set to migrate the accessions, but to match them up with the resource records, I had to use the index to search for matching id_0 fields. However, within an hour or so suddenly ASpace had updated the index, and I was all set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASpace told me I had 904 resources, which was curiously two more than we had in our master list. I assumed we failed to update our master list for two EADs and I imported them from the spreadsheet as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To confirm all the collections, I wrote &lt;code class=&quot;highlighter-rouge&quot;&gt;checkCollections.py&lt;/code&gt; to see what was up. It turned out that when we had updated that ID we still had a duplicate EAD file floating around, and another recent collection had an EAD file but was never entered in to the spreadsheet. These issues were easily fixed and all our collections were done.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;accessions-and-locations&quot;&gt;Accessions and Locations&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateAccesions.py
checkAccessions.py
migrateLocations.py
checkLocations.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The accessions and locations went easy without any issues. This time I wrote &lt;code class=&quot;highlighter-rouge&quot;&gt;checkAccessions.py&lt;/code&gt; to find all the accession issues from last time around, and everything went nicely.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some of the locations had been updated since the last import, so &lt;code class=&quot;highlighter-rouge&quot;&gt;checkLocations.py&lt;/code&gt; verifies that these locations are indeed in ArchivesSpace, prior to running the import scripts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;export-scripts&quot;&gt;Export Scripts&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exportPublicData.py
lastExport.txt
exportConverter.py
staticPages.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Once we had migrated all of our data into ArchivesSpace, it was time to set up scheduled exports and connect everything with our public access system that consists of XTF and a sets of static pages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rockefeller’s &lt;a href=&quot;https://github.com/RockefellerArchiveCenter/asExportIncremental&quot;&gt;asExportIncremental.py&lt;/a&gt; is a good source for how to do export EADs from ASpace incrementally. I followed the same basic idea, but omitted the MODS exports, and simplified it a bit into &lt;code class=&quot;highlighter-rouge&quot;&gt;exportPublicData.py&lt;/code&gt;. I integrated the same API call into the &lt;a href=&quot;https://github.com/UAlbanyArchives/archives_tools&quot;&gt;aspace library&lt;/a&gt; as &lt;code class=&quot;highlighter-rouge&quot;&gt;getResourcesSince()&lt;/code&gt; which takes a POSIX timestamp. Instead of using Pickle I just wrote the last export time to a text file called &lt;code class=&quot;highlighter-rouge&quot;&gt;lastExport.txt&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASpace’s EAD exports didn’t exactly match up with what we had been using in XTF. You can change these exports with a plugin, but I found it easier to both make some changes in our XTF XSLT files, and write a quick &lt;code class=&quot;highlighter-rouge&quot;&gt;exportConverter.py&lt;/code&gt; script with lxml.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;exportPublicData.py&lt;/code&gt; loops though all the resources modified since the last export and only exports records that are published. Since we still have a bunch of resources that only have really simple collection-level descriptions, I didn’t want to export EADs and PDFs for these, so I just exported the important information to a pipe delimited CSV.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I has always envisioned the ASpace API as completely replacing our spreadsheet that generates the static browse pages for our public access system. Yet, I didn’t want to 
export all that data over and over again, only make incremental updates like the EAD exports. Since my original static pages scripts were designed to loop though a spreadsheet, shifting to CSVs wasn’t very difficult. I also didn’t want to spend a lot of time changing things since we have some longer-term plans to export this data into a web framework.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;exportPublicData.py&lt;/code&gt; updates this CSV data for modified collections as it exports EAD XML files. It then uses Python’s &lt;a href=&quot;https://docs.python.org/3/library/subprocess.html&quot;&gt;subprocess&lt;/a&gt; module to call git commands and version these exports and pushes them to &lt;a href=&quot;https://github.com/UAlbanyArchives/collections&quot;&gt;Github&lt;/a&gt;. It then copies new EAD files to our &lt;a href=&quot;http://meg.library.albany.edu:8080/archive/search?keyword=&amp;amp;browse-all=yes&quot;&gt;public XTF instance&lt;/a&gt;. Finally, it calls &lt;code class=&quot;highlighter-rouge&quot;&gt;staticPages.py&lt;/code&gt; to create all of the &lt;a href=&quot;http://library.albany.edu/speccoll/findaids/eresources/static/apap.html&quot;&gt;static&lt;/a&gt; &lt;a href=&quot;http://library.albany.edu/speccoll/findaids/eresources/static/alpha.html&quot;&gt;browse&lt;/a&gt; &lt;a href=&quot;http://library.albany.edu/speccoll/findaids/eresources/static/subjects.html&quot;&gt;pages&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I had some encoding errors that were a pain to fix, so I ended up writing these scripts for only Python 3+.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These scripts are set up to run nightly, a few hours before XTF re-indexes. The result is that staff can make changes in ASpace, create new collections or subjects, and the new public access system pages will appear overnight, without any manual processes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;timelines&quot;&gt;Timelines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;So, how long did this all take? We got ArchivesSpace up an running on a server near the end of December 2016, really started the migration scripts during the start of February, and finished up everything on the second week of April. Most of the migration prep time spent was in February and the first week of March, and most of March was devoted to staff testing/experimenting and writing the smaller workflow tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final migration ended up taking about 2 1/2 workdays, which I’m fairly happy with. That was the only downtime we had when changes couldn’t be made.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This was also not committing a large department-wide effort. It was mostly me managing everything, and although it was probably my primary project during that period, I was also managing the regular day to day tasks of the University Archives: doing accessions and reference, managing students, and working with other offices on campus to transfer their permanent records, and also service and (not very much) scholarship. I’m also lucky to have two excellent graduate students in the University Archives this year, Ben Covell and Erik Stolarski. It really can’t be understated how having really bright and hardworking students enables us to do so much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This timeframe is also a bit deceiving, as the real work started about 2 years ago when we worked to really standardize our descriptive data. A lot of the work we did back then really shortened up our timeline, as we didn’t have to do wholesale metadata cleanup before migrating. Our data still has some issues though, but ArchivesSpace has some features like the controlled values lists that can help fix some of our irregularities after migrating. The cool thing about this is that a lot of the clean up work can be distributed, as anyone in the department can easily make fixes whenever they see something.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, we cut some corners on the data-side – particularly the locations. There are some long-term costs here as I decided to take lot of the labor required to straighten out all this data, and spread it out over time. I’m happy with this decision, as we have other competing priorities, like getting good collection-level records for everything, and overall increasing access to collections. We also could have put off the migration until some of our problems could be fixed, but there were significant maintenance costs to our old way of doing things that we really don’t have anymore. I’m hoping this frees us up to work on some of the more ambitious project we have planned.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;

&lt;p&gt;Greg Wiedeman&lt;/p&gt;</content><author><name>Gregory Wiedeman</name></author><summary type="html">This is an overview our ArchivesSpace migration process and descriptions of the code used. Hopefully this rather long-winded description helps document our processes and thinking.</summary></entry><entry><title type="html">Python for Archivists: Breaking Down Barriers Between Systems</title><link href="http://localhost:4000/2015/01/01/python-for-archivists.html" rel="alternate" type="text/html" title="Python for Archivists: Breaking Down Barriers Between Systems" /><published>2015-01-01T00:00:00-05:00</published><updated>2015-01-01T00:00:00-05:00</updated><id>http://localhost:4000/2015/01/01/python-for-archivists</id><content type="html" xml:base="http://localhost:4000/2015/01/01/python-for-archivists.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">Nicholson Baker Would Like Us to Be Angry</title><link href="http://localhost:4000/2013/03/01/nicholson-baker.html" rel="alternate" type="text/html" title="Nicholson Baker Would Like Us to Be Angry" /><published>2013-03-01T00:00:00-05:00</published><updated>2013-03-01T00:00:00-05:00</updated><id>http://localhost:4000/2013/03/01/nicholson-baker</id><content type="html" xml:base="http://localhost:4000/2013/03/01/nicholson-baker.html">&lt;p&gt;Nicholson Baker would like us to be angry—as he is—over libraries’ treatment of printed material over the last seventy years or so. His masterfully written tale is one of gross negligence, of crazed monsters masked as public servants, and of great tragedy. Baker expends great energy to reveal to the masses this stunning exposé: the purposeful destruction of invaluable public property by our most cherished institutions. Apparently, Baker tells us, our libraries have been secretly and systematically destroying all of their newspaper collections and much more. Yet when readers step back from Baker’s polemic and inquire about the availability of historic newspapers they will be shown to a microfilm machine or an online display. This, Baker insists, does not count. Here lies the crux of the matter, as Baker’s aggressive argument is a conflation of a number of issues surrounding the preservation of library and archival materials. These criticisms—some valid, some not—are mixed together indiscriminately by Baker who, despite his strong rhetoric, has essentially no understanding of the complexity and nuance that surrounds the strategy to preserve by microfilm.&lt;/p&gt;

&lt;p&gt;Of course librarians are not infallible, and it does not take a public inquiry to see the many mistakes the field has made over the previous decades. Yet, these mistakes most often are the result of tough decisions and the experimental pioneering of new technology – not a conscious vendetta against books as Baker accuses. In truth, preservation decisions such as microfilming are appraisal decisions. The appraisal process is quite imperfect and is perpetually debated by librarians and archivists. Decisions are far from above outside criticism. Yet, these decisions are made with an informed knowledge of a number of issues, and thus, carry more weight than that of an antagonistic writer with an insatiable love of books (and artificial bindings?) as artifacts. However there are very valid criticisms of the preservation appraisal decisions of the last several decades, namely librarians have almost completely ignored the access consequences in their appraisal decisions. Baker, however, is either too confused or too distracted to raise this criticism.&lt;/p&gt;

&lt;p&gt;The focus of the librarians’ actions, Baker brazenly outlines, is brittle paper. Ironically, the innovation of alum-rosin sizing that makes paper acidic and eventually brittle is what made the proliferation of newspapers possible in the first place. Twentieth-century librarians have often decided to reformat old brittle newspapers and books onto microforms with the goal of making them available permanently. In order to make the process more cost-effective, filming often required disbinding (to hasten the process) and destruction (to save storage costs).&lt;/p&gt;

&lt;p&gt;Baker protests that the disassembling and trashing of the books is not only unnecessary, but irreconcilable with libraries’ mission. He does not identify this as the preservation of the informational content and the loss of the physical content. In fact, he seems to think that these values are inseparable, stubbornly arguing that the old paper is not deteriorating, that microfilm does not effectively reproduce the text, and that microfilm does deteriorate. Baker finds that, “…wood-pulp newspapers of fifty and a hundred years ago are, contrary to incessant library propaganda, often surprisingly well preserved.”&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Yet, despite Bakers discovery, brittle paper does deteriorate, albeit very slowly. Primarily, it is use of brittle paper that causes deterioration, not just time – and thus we can see the contrasting perspectives. Librarians most often manage the material that is used the most and we can see them saddened by the destruction of poor paper, reluctantly letting at-risk material be used, and continuously seeking any way to make the information more permanent. Baker, on the other hand, opens up bound volumes of newspapers to see the intact—and likely rarely used—pages. While it is true that the deterioration process can be a very slow one, Baker will not find the copies that have failed, for obvious reasons. Librarians want material to be available for a large volume of use, both careful and careless, for a very long time. This is a mission that brittle paper often cannot fulfill.&lt;/p&gt;

&lt;p&gt;Our polemist also points out that microforms are often not up to this task as well. First, he charges that they do serve as adequate surrogates for the original, failing to reproduce images and making the text difficult to make out. He charges that significant amounts of texts are lost in the process. He then details the various problems with different types of microform, like vinegar syndrome, before declaring even modern archival-quality microfilm insufficient. Some of the lesser microfilm, he complains, will not stand 175 degree temperatures, but he fails to mention the effect those conditions would have on brittle paper.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; While Baker points out that microforms—particularly older types—are not truly permanent either, they do hold up much better than poor paper with significant use in a good environment. Also, it is important to note that is easier to provide a good preservation environment for a run of newspapers on microfilm than on paper.&lt;/p&gt;

&lt;p&gt;Baker is also unnecessarily critical of film being used as a stopgap before digitization. He states that, “…attempts to scan the page-images of newspapers from old microfilm have not worked well—and will never work well—because microfilm itself is often at the squint-to-make-out level.”&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; However modern filming with sufficient quality control produces images that can reproduce the text quite well. Microfilm readers, however, are more problematic. Contrary to Baker’s belief, microfilm to 1s and 0s is now quite commonplace, and cheaper than scanning from the originals. The Franklin D. Roosevelt Library has problems with extensive use of its collections. Repeated handling has done permanent damage to both good paper and acidic paper. Until recently, the preservation microfilm that has been performed over the years has gone unused as patrons preferred the originals. Now they are undergoing an extensive project to digitize multiple collections from microfilm. The images being produced are not preservation-quality digital images, yet the increased access with online availability have made these digital images preferable for researchers.&lt;/p&gt;

&lt;p&gt;Perhaps the worst aspect of Baker’s book is his use of harsh hyperbolic language and unfair depictions of librarians. The perceptive reader will realize that powerful language does not replace evidence and historians will be disgusted with Baker’s appalling over-the-top demonization. Here we have “innocently trusted,” public servants on a campaign of, “methodical eradication,” “general devastation,” and, “annihilation,” that has, “…strip-mined a hundred and twenty years of its history.”&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; Did Baker think readers would embrace the notion that our libraries are run by a conspiring cadre (who apparently think of themselves as movie studio moguls) who, “shamelessly lied,” and love, “petty vandalism”?&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; While he avoids outwardly proclaiming a conspiracy he certainly preys upon its major themes, mongering that, “None of this epochal activity, in which the Library of Congress began its slow betrayal of an unknowing nation, was published in contemporary annual reports.” Yet, a page later he states that it did list the newspapers in itsInformation Bulletin and never tries to reconcile how conniving librarians so unabashedly reveal their aims elsewhere.&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; In is discussion of Cold War librarians he often juxtaposes random examples of craziness with library experiments as a key part of his argument. While Fremont Rider offers his (admittedly unscientific) estimates of library growth we are told that Rider also had written science fiction.&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; Baker states that he is unsure of whether William James Barrow actually believed his simulated aging estimates, then cites an interviewee who states that many of these numbers, “…were made to get people’s attention.”&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; This is a duplicitous way of arguing without evidence: perhaps Baker actually believed his argument; some would say he was writing to get people’s attention. To quote Baker: “This is of course utter horseshit and craziness.”&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Our polemist always avoids nuanced and empathetical examinations of the figures in question, instead choosing to suggest purposeful maliciousness. The past is a foreign country, as they say, and it is exceedingly difficult to understand the culture and intellectual environment in which decisions were made. Baker fails to comprehend the positivism and faith in technology that pervaded the minds of librarians decades ago. While that does not excuse their poor judgments, it does mean that those decisions must be critiqued within that context. Isolating those choices, arguing without evidence, and assuming malicious intent is disingenuous and duplicitous. The librarians of the Cold War were driven by the immense fear that the books will be gone – the same fear as Nicholson Baker.&lt;/p&gt;

&lt;p&gt;There are some very valid criticisms preservation decisions of Cold War librarians, if only Baker recognized them. The most important failing has been the ignorance of access in preservation decisions. Despite the assertions of many forward-thinking librarians in the 1950s, microfilm is not nearly as effective a medium as paper books. While film effectively captures intellectual content, it acts as a tremendous barrier because it is more difficult to use. Many patrons such as Baker will be disappointed if the material they seek is only available on microfilm – some will seek another avenue rather than deal with the time-consuming, uncomfortable, and difficult-to-read microfilm reader. Microfilm cannot be checked out and read in the comfort of a researcher’s own home at his or her own schedule. Librarians (and archivists) have yet again focused on their holdings rather than their people.&lt;/p&gt;

&lt;p&gt;Yet, dependence on microforms has not led to the apocalypse that Baker asserts. Microfilm has not, “…undermined American historiography far more seriously than anything that alum-tormented newsprint could possibly have done to itself.”&lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; On the contrary, these lost newspapers are still the most widely-used sources for American historians. Changes in historical theory have ended newspapers’ place as central pieces of evidence, yet one would be hard pressed to find a recent major work of American history that does not cite newspapers in some manner. They are comprehensive, widely accessible because of digitization, and good for context and cross-referencing. Contrary to what Baker thinks, academic historians have no time or value for reading newspapers, “…page by page month by month for pleasure…”&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; Every academic historian will be laugh at Baker’s assertion. They care little about “…gorgeously drippy art-nouveau graphics,” or “elaborately hand lettered ornamental headlines,” unless researching newspaper art history.&lt;sup id=&quot;fnref:12&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; Whatever Baker thinks, that interest is so obscure that it hardly registers on archivists’ long list of appraisal priorities.&lt;/p&gt;

&lt;p&gt;In some cases, microfilm reproductions can me more accessible and more useful than the originals. Baker decries Barrow’s testing on the works of Bayard Taylor and William Cooper Prime, stating that, “Some of these books would be nice to have now…”&lt;sup id=&quot;fnref:13&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; While he would like us to think that these texts are lost, travel writings like these were so popular in the 1850s that one still stumbles upon them in old bookstores. Multiple copies are digitized and available with keyword search on Google Books or as ebooks from the Project Gutenberg. In one case a graduate student&lt;sup id=&quot;fnref:14&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; researching the topic preferred a physical book. Luckily, facsimile reproductions from microfilm are cheaply purchased online and wholly readable on acid-free paper and good bindings. In one case, the graduate student purchased a copy of George William Curtis’s The Howaji in Syria for under $5 and unexpectedly received an 1865 original Harper &amp;amp; Brothers copy. The brittle paper and poor binding made this copy fairly unusable and definitively lesser than the reproductions.&lt;/p&gt;

&lt;p&gt;Baker wants to keep every original published work, including each edition and alteration. Not only is he unable to grasp the sheer physics of a research library keeping every published work, he is seeking a comprehensive record of knowledge that simply does not exist. Preservation appraisal is not only about intrinsic value, but intellectual content, accessibility, feasibility, and a number of other concerns that Baker has not taken the time to understand despite interviewing scores of librarians and archivists. Intrinsic or artificial value seems simple, and objective to Baker, but it is nothing of the sort. Like any other aspect of culture, it is different for everyone, changes routinely, and often dramatically. Conservator Jane Smith might value the texture of 17th century paper, but the paper’s contemporaries certainly did not.&lt;sup id=&quot;fnref:15&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; There are very real and valid criticisms within Baker’s rant. Too bad he does not understand them.
Notes&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Nicholson Baker, Double Fold: Libraries and the Assault on Paper (New York: Random House, 2001), 5. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Ibid., 43. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Ibid., 16. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Ibid, 13, 16, 16, 18, 20. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Ibid., 27, 41, 79. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Ibid., 32, 33. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;Ibid., 77. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;ibid., 143. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;Ibid., 157. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot;&gt;
      &lt;p&gt;Ibid., 147. &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;Ibid., 39. &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot;&gt;
      &lt;p&gt;Ibid., 4. &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot;&gt;
      &lt;p&gt;Ibid., 154. &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot;&gt;
      &lt;p&gt;Me. &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot;&gt;
      &lt;p&gt;Baker, 150. &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Gregory Wiedeman</name></author><summary type="html">Nicholson Baker would like us to be angry—as he is—over libraries’ treatment of printed material over the last seventy years or so. His masterfully written tale is one of gross negligence, of crazed monsters masked as public servants, and of great tragedy. Baker expends great energy to reveal to the masses this stunning exposé: the purposeful destruction of invaluable public property by our most cherished institutions. Apparently, Baker tells us, our libraries have been secretly and systematically destroying all of their newspaper collections and much more. Yet when readers step back from Baker’s polemic and inquire about the availability of historic newspapers they will be shown to a microfilm machine or an online display. This, Baker insists, does not count. Here lies the crux of the matter, as Baker’s aggressive argument is a conflation of a number of issues surrounding the preservation of library and archival materials. These criticisms—some valid, some not—are mixed together indiscriminately by Baker who, despite his strong rhetoric, has essentially no understanding of the complexity and nuance that surrounds the strategy to preserve by microfilm. Of course librarians are not infallible, and it does not take a public inquiry to see the many mistakes the field has made over the previous decades. Yet, these mistakes most often are the result of tough decisions and the experimental pioneering of new technology – not a conscious vendetta against books as Baker accuses. In truth, preservation decisions such as microfilming are appraisal decisions. The appraisal process is quite imperfect and is perpetually debated by librarians and archivists. Decisions are far from above outside criticism. Yet, these decisions are made with an informed knowledge of a number of issues, and thus, carry more weight than that of an antagonistic writer with an insatiable love of books (and artificial bindings?) as artifacts. However there are very valid criticisms of the preservation appraisal decisions of the last several decades, namely librarians have almost completely ignored the access consequences in their appraisal decisions. Baker, however, is either too confused or too distracted to raise this criticism. The focus of the librarians’ actions, Baker brazenly outlines, is brittle paper. Ironically, the innovation of alum-rosin sizing that makes paper acidic and eventually brittle is what made the proliferation of newspapers possible in the first place. Twentieth-century librarians have often decided to reformat old brittle newspapers and books onto microforms with the goal of making them available permanently. In order to make the process more cost-effective, filming often required disbinding (to hasten the process) and destruction (to save storage costs). Baker protests that the disassembling and trashing of the books is not only unnecessary, but irreconcilable with libraries’ mission. He does not identify this as the preservation of the informational content and the loss of the physical content. In fact, he seems to think that these values are inseparable, stubbornly arguing that the old paper is not deteriorating, that microfilm does not effectively reproduce the text, and that microfilm does deteriorate. Baker finds that, “…wood-pulp newspapers of fifty and a hundred years ago are, contrary to incessant library propaganda, often surprisingly well preserved.”1 Yet, despite Bakers discovery, brittle paper does deteriorate, albeit very slowly. Primarily, it is use of brittle paper that causes deterioration, not just time – and thus we can see the contrasting perspectives. Librarians most often manage the material that is used the most and we can see them saddened by the destruction of poor paper, reluctantly letting at-risk material be used, and continuously seeking any way to make the information more permanent. Baker, on the other hand, opens up bound volumes of newspapers to see the intact—and likely rarely used—pages. While it is true that the deterioration process can be a very slow one, Baker will not find the copies that have failed, for obvious reasons. Librarians want material to be available for a large volume of use, both careful and careless, for a very long time. This is a mission that brittle paper often cannot fulfill. Our polemist also points out that microforms are often not up to this task as well. First, he charges that they do serve as adequate surrogates for the original, failing to reproduce images and making the text difficult to make out. He charges that significant amounts of texts are lost in the process. He then details the various problems with different types of microform, like vinegar syndrome, before declaring even modern archival-quality microfilm insufficient. Some of the lesser microfilm, he complains, will not stand 175 degree temperatures, but he fails to mention the effect those conditions would have on brittle paper.2 While Baker points out that microforms—particularly older types—are not truly permanent either, they do hold up much better than poor paper with significant use in a good environment. Also, it is important to note that is easier to provide a good preservation environment for a run of newspapers on microfilm than on paper. Baker is also unnecessarily critical of film being used as a stopgap before digitization. He states that, “…attempts to scan the page-images of newspapers from old microfilm have not worked well—and will never work well—because microfilm itself is often at the squint-to-make-out level.”3 However modern filming with sufficient quality control produces images that can reproduce the text quite well. Microfilm readers, however, are more problematic. Contrary to Baker’s belief, microfilm to 1s and 0s is now quite commonplace, and cheaper than scanning from the originals. The Franklin D. Roosevelt Library has problems with extensive use of its collections. Repeated handling has done permanent damage to both good paper and acidic paper. Until recently, the preservation microfilm that has been performed over the years has gone unused as patrons preferred the originals. Now they are undergoing an extensive project to digitize multiple collections from microfilm. The images being produced are not preservation-quality digital images, yet the increased access with online availability have made these digital images preferable for researchers. Perhaps the worst aspect of Baker’s book is his use of harsh hyperbolic language and unfair depictions of librarians. The perceptive reader will realize that powerful language does not replace evidence and historians will be disgusted with Baker’s appalling over-the-top demonization. Here we have “innocently trusted,” public servants on a campaign of, “methodical eradication,” “general devastation,” and, “annihilation,” that has, “…strip-mined a hundred and twenty years of its history.”4 Did Baker think readers would embrace the notion that our libraries are run by a conspiring cadre (who apparently think of themselves as movie studio moguls) who, “shamelessly lied,” and love, “petty vandalism”?5 While he avoids outwardly proclaiming a conspiracy he certainly preys upon its major themes, mongering that, “None of this epochal activity, in which the Library of Congress began its slow betrayal of an unknowing nation, was published in contemporary annual reports.” Yet, a page later he states that it did list the newspapers in itsInformation Bulletin and never tries to reconcile how conniving librarians so unabashedly reveal their aims elsewhere.6 In is discussion of Cold War librarians he often juxtaposes random examples of craziness with library experiments as a key part of his argument. While Fremont Rider offers his (admittedly unscientific) estimates of library growth we are told that Rider also had written science fiction.7 Baker states that he is unsure of whether William James Barrow actually believed his simulated aging estimates, then cites an interviewee who states that many of these numbers, “…were made to get people’s attention.”8 This is a duplicitous way of arguing without evidence: perhaps Baker actually believed his argument; some would say he was writing to get people’s attention. To quote Baker: “This is of course utter horseshit and craziness.”9 Our polemist always avoids nuanced and empathetical examinations of the figures in question, instead choosing to suggest purposeful maliciousness. The past is a foreign country, as they say, and it is exceedingly difficult to understand the culture and intellectual environment in which decisions were made. Baker fails to comprehend the positivism and faith in technology that pervaded the minds of librarians decades ago. While that does not excuse their poor judgments, it does mean that those decisions must be critiqued within that context. Isolating those choices, arguing without evidence, and assuming malicious intent is disingenuous and duplicitous. The librarians of the Cold War were driven by the immense fear that the books will be gone – the same fear as Nicholson Baker. There are some very valid criticisms preservation decisions of Cold War librarians, if only Baker recognized them. The most important failing has been the ignorance of access in preservation decisions. Despite the assertions of many forward-thinking librarians in the 1950s, microfilm is not nearly as effective a medium as paper books. While film effectively captures intellectual content, it acts as a tremendous barrier because it is more difficult to use. Many patrons such as Baker will be disappointed if the material they seek is only available on microfilm – some will seek another avenue rather than deal with the time-consuming, uncomfortable, and difficult-to-read microfilm reader. Microfilm cannot be checked out and read in the comfort of a researcher’s own home at his or her own schedule. Librarians (and archivists) have yet again focused on their holdings rather than their people. Yet, dependence on microforms has not led to the apocalypse that Baker asserts. Microfilm has not, “…undermined American historiography far more seriously than anything that alum-tormented newsprint could possibly have done to itself.”10 On the contrary, these lost newspapers are still the most widely-used sources for American historians. Changes in historical theory have ended newspapers’ place as central pieces of evidence, yet one would be hard pressed to find a recent major work of American history that does not cite newspapers in some manner. They are comprehensive, widely accessible because of digitization, and good for context and cross-referencing. Contrary to what Baker thinks, academic historians have no time or value for reading newspapers, “…page by page month by month for pleasure…”11 Every academic historian will be laugh at Baker’s assertion. They care little about “…gorgeously drippy art-nouveau graphics,” or “elaborately hand lettered ornamental headlines,” unless researching newspaper art history.12 Whatever Baker thinks, that interest is so obscure that it hardly registers on archivists’ long list of appraisal priorities. In some cases, microfilm reproductions can me more accessible and more useful than the originals. Baker decries Barrow’s testing on the works of Bayard Taylor and William Cooper Prime, stating that, “Some of these books would be nice to have now…”13 While he would like us to think that these texts are lost, travel writings like these were so popular in the 1850s that one still stumbles upon them in old bookstores. Multiple copies are digitized and available with keyword search on Google Books or as ebooks from the Project Gutenberg. In one case a graduate student14 researching the topic preferred a physical book. Luckily, facsimile reproductions from microfilm are cheaply purchased online and wholly readable on acid-free paper and good bindings. In one case, the graduate student purchased a copy of George William Curtis’s The Howaji in Syria for under $5 and unexpectedly received an 1865 original Harper &amp;amp; Brothers copy. The brittle paper and poor binding made this copy fairly unusable and definitively lesser than the reproductions. Baker wants to keep every original published work, including each edition and alteration. Not only is he unable to grasp the sheer physics of a research library keeping every published work, he is seeking a comprehensive record of knowledge that simply does not exist. Preservation appraisal is not only about intrinsic value, but intellectual content, accessibility, feasibility, and a number of other concerns that Baker has not taken the time to understand despite interviewing scores of librarians and archivists. Intrinsic or artificial value seems simple, and objective to Baker, but it is nothing of the sort. Like any other aspect of culture, it is different for everyone, changes routinely, and often dramatically. Conservator Jane Smith might value the texture of 17th century paper, but the paper’s contemporaries certainly did not.15 There are very real and valid criticisms within Baker’s rant. Too bad he does not understand them. Notes Nicholson Baker, Double Fold: Libraries and the Assault on Paper (New York: Random House, 2001), 5. &amp;#8617; Ibid., 43. &amp;#8617; Ibid., 16. &amp;#8617; Ibid, 13, 16, 16, 18, 20. &amp;#8617; Ibid., 27, 41, 79. &amp;#8617; Ibid., 32, 33. &amp;#8617; Ibid., 77. &amp;#8617; ibid., 143. &amp;#8617; Ibid., 157. &amp;#8617; Ibid., 147. &amp;#8617; Ibid., 39. &amp;#8617; Ibid., 4. &amp;#8617; Ibid., 154. &amp;#8617; Me. &amp;#8617; Baker, 150. &amp;#8617;</summary></entry></feed>