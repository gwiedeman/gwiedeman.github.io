<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-05-16T15:54:24-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Greg Wiedeman</title><subtitle>I'm an archivist who works to make information more accessible.</subtitle><author><name>Gregory Wiedeman</name></author><entry><title type="html">Designing Digital Discovery and Access Systems for Archival Description</title><link href="http://localhost:4000/2023/01/20/designing-digital-discovery.html" rel="alternate" type="text/html" title="Designing Digital Discovery and Access Systems for Archival Description" /><published>2023-01-20T00:00:00-05:00</published><updated>2023-01-20T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/20/designing-digital-discovery</id><content type="html" xml:base="http://localhost:4000/2023/01/20/designing-digital-discovery.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">Why Blacklight and Arclight are Awesome</title><link href="http://localhost:4000/2022/12/16/blacklight-arclight.html" rel="alternate" type="text/html" title="Why Blacklight and Arclight are Awesome" /><published>2022-12-16T00:00:00-05:00</published><updated>2022-12-16T00:00:00-05:00</updated><id>http://localhost:4000/2022/12/16/blacklight-arclight</id><content type="html" xml:base="http://localhost:4000/2022/12/16/blacklight-arclight.html">&lt;p&gt;Blacklight and Arclight might be my favorite tools right now.  They have many of the characteristics in tools and systems that I really enjoy: they do one thing well, they‚Äôre super extensible and customizable, and you feel like you have superpowers when you use them. Yet, what I‚Äôm really loving about them is how they are making me think about how we could design better access and discovery systems in libraries and archives, particularly for small and medium-sized institutions and regional consortiums. I think they have the potential to better match our systems to our skillsets and organizational structures by empowering librarians and archivists to design and manage digital repositories at a detailed technical level.&lt;/p&gt;

&lt;p&gt;When we think of digital repositories, we usually think of monolithic web applications. A system that your institution or consortium implements or pays to host that gives you a web form to upload files and enter metadata. Users see a search box on the front end that returns and displays digital objects and metadata. Hyrax. Hyku. Samvera. Islandora. DSpace. ContentDM. Bepress Digital Commons. Pick one. If you just need an Institutional Repository (IR) to host PDFs and make them discoverable, they‚Äôre great. Like any piece of software, when you ask them to do more than they‚Äôre designed for, you run into trouble. Chances are your library or archive has other digital assets with a variety of processing workflows and preservation needs beyond ‚Äúit‚Äôs in Fedora.‚Äù&lt;/p&gt;

&lt;p&gt;Digital repositories also &lt;em&gt;&lt;u&gt;cost so much&lt;/u&gt;&lt;/em&gt;, either literally or in developer time. I‚Äôve implemented a lot of systems and digital repositories have easily been the most challenging. ArchivesSpace? No problem. Islandora? üò¨.&lt;/p&gt;

&lt;p&gt;Repositories just have a lot going on under the hood. They save files to a file system (directly or through Fedora), often making multiple copies. They make thumbnails, extracting text with Apache Tika, OCR. Tools upon tools upon tools. Much of this isn‚Äôt complicated individually, but when you put all this collectively and orchestrate it from behind a web form it gets hard quick. What version does that middleware support? You probably need file versioning and that gets &lt;em&gt;&lt;u&gt;really complicated&lt;/u&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Traditional monolithic digital repositories might be a fit if you have a department of developers and applications support folks with skills in a particular software stack, like Rails or Drupal. This is a &lt;em&gt;&lt;u&gt;really small&lt;/u&gt;&lt;/em&gt; portion of institutions in the US. I work in a (purportedly) ARL library at a (purportedly) R1 university. Even before the massive staff cuts we didn‚Äôt have anywhere near these resources. College and university archives are also likely to be much better funded than public archives. State and local archives probably have more important collecting scopes and do really impactful work, and really struggle providing access to digital materials. This is without even talking about community archives. For the vast majority of libraries and archives, you just pay a large hosting bill or membership fee and get the system you get, and that‚Äôs if you‚Äôre lucky enough to have a digital repository at all.&lt;/p&gt;

&lt;p&gt;Despite staff cuts throughout the profession, we still have a lot of great people working in libraries and archives, from the largest institutions to the one person in the county archives. They usually don‚Äôt work in code, but they can totally figure out how to make thumbnails of images and understand why this is useful. They usually have a really strong understanding of what a digital repository or other system is supposed to be doing, a much better understanding than developers do. A large part of building software is cross-domain communication. There‚Äôs a whole Systems Design discipline focused on developers asking questions and understanding the needs of practitioners. Navigating this boundary is one of the most challenging parts of software development. Instead of this solid line between developers and librarians and archivists, what if we built and managed our repository systems together? Not just by developing system requirements, but by having librarians and archivists directly organize and manage all the data.&lt;/p&gt;

&lt;p&gt;I think Blacklight and Arclight can make this happen. Both of these tools merely provide a front-end web interface for data stored in Apache Solr. They provide a search box to query Solr for results, provide facets and other limiters with buttons or drop downs, and show a page for each item. Once your data is in Solr, you can just edit what is essentially a config file in Blacklight or Arclight and tell it what fields your data has, what fields should be facets and what fields should display on the item pages. They have some customizable templates that affect what things look like and you need a basic understanding of Solr field types, but essentially that‚Äôs it. Arclight has some more complex templates as it‚Äôs set up to display hierarchical archives data, but functionally it does the same thing. If you have a thumbnail URL in your data, you enter that field in the Blacklight config and it will display thumbnails. If you include a URL to a PDF or IIIF manifest in the Solr data and you can edit a template to display or download it.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This means that if just we get our data into Solr, Blacklight or Arclight will provide access to it. And there are a number of ways to get data into Solr. You can use the &lt;a href=&quot;https://solr.apache.org/guide/8_5/post-tool.html&quot;&gt;command line post tool&lt;/a&gt;, or send it JSON data with &lt;a href=&quot;https://stackoverflow.com/questions/49353549/sending-json-data-to-solr-using-curl&quot;&gt;a curl POST request&lt;/a&gt;. There are libraries that send data to Solr for &lt;a href=&quot;https://github.com/django-haystack/pysolr/&quot;&gt;Python&lt;/a&gt;, &lt;a href=&quot;https://github.com/rsolr/rsolr&quot;&gt;Ruby&lt;/a&gt;, and it looks like there‚Äôs a bunch for JavaScript. If Solr is configured for it, you can also send Solr office documents or PDFs and it will extract text from it, or you can use something like &lt;a href=&quot;https://tika.apache.org/&quot;&gt;Apache Tika&lt;/a&gt; to extract text and include it in the JSON you send to Solr. If our data is structured, consistent, and machine-actionable its relatively easy to build small utilities or workflow tools that read data and send it to Solr. This is not nothing and, along with supporting Blacklight or Arclight and Solr, still takes developer-time, but these challenges might be significantly easier than designing or even maintaining a system like Hyrax which wraps Blacklight and data management functionality in one much more complex application.&lt;/p&gt;

&lt;p&gt;So, from a technical level, separating out Solr and the front end access system from how data is managed should be quite feasible. This requires really consistent and effective file and data management in some kind of open system. This is really hard, but it‚Äôs also work that many librarians and archivists are great at. Most librarians and archivists are also constantly frustrated at the friction that our systems create and would jump at the chance to design data management systems if we can use tools they are already comfortable with.&lt;/p&gt;

&lt;p&gt;To this end, &lt;strong&gt;I want to design digital repositories around spreadsheets. File systems. Google Drive.&lt;/strong&gt; These are all well-structured and open enough to be accessed by small utilities to read and POST data into Solr. This requires strong data models and detailed specifications that structure our data at a really granular level. Let‚Äôs build them together. I really like the &lt;a href=&quot;https://www.rfc-editor.org/rfc/rfc8493&quot;&gt;Bagit specification&lt;/a&gt;, which inspired &lt;a href=&quot;https://archives.albany.edu/mailbag/spec/&quot;&gt;Mailbag&lt;/a&gt;. I‚Äôve also enjoyed using &lt;a href=&quot;https://github.com/jazzband/jsonmodels&quot;&gt;jsonmodels&lt;/a&gt; for writing super-simple JSON data models, and I‚Äôm using it to write &lt;a href=&quot;https://github.com/UAlbanyArchives/description_indexer/blob/dao-indexing/description_indexer/models/description.py&quot;&gt;a basic data model for digital objects and files in archival description&lt;/a&gt;. Maybe the filesystem specifications could be based on the &lt;a href=&quot;https://ocfl.io/&quot;&gt;Oxford Common File Layout&lt;/a&gt; (OCFL). Here‚Äôs a quick-and-dirty example of vaguely what I‚Äôm thinking based on Bagit:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;
	&lt;pre class=&quot;highlight text&quot;&gt;
		&lt;code&gt;
  &amp;lt;digital object&amp;gt;/
     |
     -- metadata.yml
     |
     -- bag-info.txt
     |
     -- manifest-&amp;lt;algorithm&amp;gt;.txt
     |
     -- tagmanifest-&amp;lt;algorithm&amp;gt;.txt
     |
     +-- files/
           |
           +-- &amp;lt;file&amp;gt;/
           |     -- content.txt
           |     -- metadata.yml
           |     -- thumbnail.jpg
           |     +-- versions/
           |             +-- &amp;lt;version&amp;gt;/
           |     	 |      -- metadata.yml
           |     	 |      -- &amp;lt;access file&amp;gt;.jpg
           |             +-- &amp;lt;version&amp;gt;/
           |     	        -- metadata.yml
           |     	        -- &amp;lt;original file&amp;gt;.tif
           +-- &amp;lt;file&amp;gt;/
                 ...
		&lt;/code&gt;
	&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Essentially, this data controls what gets sent to Solr, which controls what is displayed and made accessible in Blacklight and Arclight. Which means &lt;strong&gt;librarians and archivists can control our access systems by copying and pasting files and editing text files and spreadsheets.&lt;/strong&gt; We should start by ensuring that this work can be done manually, and then slowly build as-simple-as-possible tools and utilities to automate labor-intensive tasks. Maybe these are command line scripts, maybe they‚Äôre super simple Electron or Flask apps. &lt;a href=&quot;https://github.com/adamkewley/jobson&quot;&gt;Jobson&lt;/a&gt; looks cool. Need a web form to create objects? That could be an optional single page app. Using common tools like filesystems and spreadsheets that people use everyday outside of our profession makes it more likely that we could use or adapt a variety of workflow tools that may be developed for different purposes. This would ensure that our practices are extensible from small repositories that can still do the work manually to large institutions working at scale. This also puts us all on the same foundation making it more likely that tools and processes developed at larger repositories could actually benefit smaller archives.&lt;/p&gt;

&lt;p&gt;This model also fits regional consortiums that support small repositories well. We can index from a common layout in Google Drive! That could be a bit clunky, but feasible! Consortia can better host common large applications like Solr, Blacklight/Arclight, and image servers while local institutions manage their own data. If the data is structured and managed well enough according to a common specification, consortia can read (almost crawl?) and index the data from a couple different sources, including commercial cloud storage. This allows them to meet small archives wherever they are at. The reverse could also work too, as medium-sized archives could use tools or scripts to read their data and POST it into a consortia Solr.&lt;/p&gt;

&lt;p&gt;What I also like about this is that you also don‚Äôt need to be an instant expert to participate. When I started learning how to do this work, I was writing XML by hand. Now the grad students I work with only see the shiny web forms in ArchivesSpace or Hyrax. It‚Äôs has become so much harder to onboard new professionals and have them contribute to access in a meaningful way. A manual-first access system facilitates step-by-step learning where we can all start from the same foundation and adopt individual tools one-by-one over time.&lt;/p&gt;

&lt;p&gt;There are still major challenges here. Quality specifications and data models take a lot of community consensus-building. Validation is a concern, although we can build simple tools that allow librarians and archivists to validate and evaluate their own data. I also haven‚Äôt fully wrapped my head around how versioning might work in a way that accommodates my practical local examples, what OCFL is doing, and the spirit of &lt;a href=&quot;https://en.wikipedia.org/wiki/Open_Archival_Information_System&quot;&gt;OAIS&lt;/a&gt;. How would we get access files to an image server? From a preservation risk perspective, it‚Äôs also easy to delete a lot manually. But read-only filesystems exist, and we should be able to design systems with substantial fault tolerance. I‚Äôd also argue that separating file and data management from complex monolithic tools also eliminates significant preservation risk. There are problems than would need to be addressed, but these are types of challenges that librarians and archivists are well equipped to take on in collaboration with technologists.&lt;/p&gt;

&lt;p&gt;Overall, I‚Äôm really intrigued with the design scope of Blacklight and Arclight because they make it possible for librarians, archivists, and technologists with a wide variety of expertise and experiences to design and build access systems together. If we use common, everyday tools like filesystems, text files, spreadsheets, network file shares, and commercial cloud storage for file and data management, librarians and archivists can take on this work, allowing them to be equal partners with technologists in building access systems for digital materials.&lt;/p&gt;

&lt;p&gt;I think this would also better match this work with our organizational structures, as currently system design is often either too concentrated in technology departments, or design is done in practice by library leadership selecting vendors. I also think most libraries don‚Äôt have a good process of evaluating vendor services and base much of this on reputation and branding.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Separating data management from digital repositories and using common tools could allow even smaller libraries and archives to retain full control of their data. There can be roles for vendors, but the path I am envisioning relegates them to individual tools. Hosting Solr or the front-end? Sure. A vendor OCR or transcription service? Sign me up. I‚Äôm sure vendors will fill in gaps and help with particularly painful processes.&lt;/p&gt;

&lt;p&gt;Designing access systems around filesystems and spreadsheets requires strong specifications and data models, which pushes this design work in small and medium-sized archives away from administrators (who have different skillsets) to practitioners‚Äîlibrarians, archivists, and technologists‚Äîwho work directly with materials and core services. This work can then be done collaboratively and collectively, with everyone contributing their expertise. Let‚Äôs match our systems to our organizational strengths.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;If you have some coding experience and want to try out Blacklight first-hand, &lt;a href=&quot;https://www.nopio.com/blog/create-search-application-blacklight/&quot;&gt;this tutorial&lt;/a&gt; was really helpful for me.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;I feel like there‚Äôs this phenomenon in libraries where the technologists have too much power in how our systems and services work, so librarians and archivists in turn marginalize technologists in planning and policymaking. Now, I have heard about places where technologists were put in charge of a special collections department or academic library, which might be the darkest timeline, so some of this might be rational defense, but there‚Äôs also definitely a role for technologists in planning and policymaking!¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Gregory Wiedeman</name></author><summary type="html">Blacklight and Arclight might be my favorite tools right now. They have many of the characteristics in tools and systems that I really enjoy: they do one thing well, they‚Äôre super extensible and customizable, and you feel like you have superpowers when you use them. Yet, what I‚Äôm really loving about them is how they are making me think about how we could design better access and discovery systems in libraries and archives, particularly for small and medium-sized institutions and regional consortiums. I think they have the potential to better match our systems to our skillsets and organizational structures by empowering librarians and archivists to design and manage digital repositories at a detailed technical level. When we think of digital repositories, we usually think of monolithic web applications. A system that your institution or consortium implements or pays to host that gives you a web form to upload files and enter metadata. Users see a search box on the front end that returns and displays digital objects and metadata. Hyrax. Hyku. Samvera. Islandora. DSpace. ContentDM. Bepress Digital Commons. Pick one. If you just need an Institutional Repository (IR) to host PDFs and make them discoverable, they‚Äôre great. Like any piece of software, when you ask them to do more than they‚Äôre designed for, you run into trouble. Chances are your library or archive has other digital assets with a variety of processing workflows and preservation needs beyond ‚Äúit‚Äôs in Fedora.‚Äù Digital repositories also cost so much, either literally or in developer time. I‚Äôve implemented a lot of systems and digital repositories have easily been the most challenging. ArchivesSpace? No problem. Islandora? üò¨. Repositories just have a lot going on under the hood. They save files to a file system (directly or through Fedora), often making multiple copies. They make thumbnails, extracting text with Apache Tika, OCR. Tools upon tools upon tools. Much of this isn‚Äôt complicated individually, but when you put all this collectively and orchestrate it from behind a web form it gets hard quick. What version does that middleware support? You probably need file versioning and that gets really complicated. Traditional monolithic digital repositories might be a fit if you have a department of developers and applications support folks with skills in a particular software stack, like Rails or Drupal. This is a really small portion of institutions in the US. I work in a (purportedly) ARL library at a (purportedly) R1 university. Even before the massive staff cuts we didn‚Äôt have anywhere near these resources. College and university archives are also likely to be much better funded than public archives. State and local archives probably have more important collecting scopes and do really impactful work, and really struggle providing access to digital materials. This is without even talking about community archives. For the vast majority of libraries and archives, you just pay a large hosting bill or membership fee and get the system you get, and that‚Äôs if you‚Äôre lucky enough to have a digital repository at all. Despite staff cuts throughout the profession, we still have a lot of great people working in libraries and archives, from the largest institutions to the one person in the county archives. They usually don‚Äôt work in code, but they can totally figure out how to make thumbnails of images and understand why this is useful. They usually have a really strong understanding of what a digital repository or other system is supposed to be doing, a much better understanding than developers do. A large part of building software is cross-domain communication. There‚Äôs a whole Systems Design discipline focused on developers asking questions and understanding the needs of practitioners. Navigating this boundary is one of the most challenging parts of software development. Instead of this solid line between developers and librarians and archivists, what if we built and managed our repository systems together? Not just by developing system requirements, but by having librarians and archivists directly organize and manage all the data. I think Blacklight and Arclight can make this happen. Both of these tools merely provide a front-end web interface for data stored in Apache Solr. They provide a search box to query Solr for results, provide facets and other limiters with buttons or drop downs, and show a page for each item. Once your data is in Solr, you can just edit what is essentially a config file in Blacklight or Arclight and tell it what fields your data has, what fields should be facets and what fields should display on the item pages. They have some customizable templates that affect what things look like and you need a basic understanding of Solr field types, but essentially that‚Äôs it. Arclight has some more complex templates as it‚Äôs set up to display hierarchical archives data, but functionally it does the same thing. If you have a thumbnail URL in your data, you enter that field in the Blacklight config and it will display thumbnails. If you include a URL to a PDF or IIIF manifest in the Solr data and you can edit a template to display or download it.1 This means that if just we get our data into Solr, Blacklight or Arclight will provide access to it. And there are a number of ways to get data into Solr. You can use the command line post tool, or send it JSON data with a curl POST request. There are libraries that send data to Solr for Python, Ruby, and it looks like there‚Äôs a bunch for JavaScript. If Solr is configured for it, you can also send Solr office documents or PDFs and it will extract text from it, or you can use something like Apache Tika to extract text and include it in the JSON you send to Solr. If our data is structured, consistent, and machine-actionable its relatively easy to build small utilities or workflow tools that read data and send it to Solr. This is not nothing and, along with supporting Blacklight or Arclight and Solr, still takes developer-time, but these challenges might be significantly easier than designing or even maintaining a system like Hyrax which wraps Blacklight and data management functionality in one much more complex application. So, from a technical level, separating out Solr and the front end access system from how data is managed should be quite feasible. This requires really consistent and effective file and data management in some kind of open system. This is really hard, but it‚Äôs also work that many librarians and archivists are great at. Most librarians and archivists are also constantly frustrated at the friction that our systems create and would jump at the chance to design data management systems if we can use tools they are already comfortable with. To this end, I want to design digital repositories around spreadsheets. File systems. Google Drive. These are all well-structured and open enough to be accessed by small utilities to read and POST data into Solr. This requires strong data models and detailed specifications that structure our data at a really granular level. Let‚Äôs build them together. I really like the Bagit specification, which inspired Mailbag. I‚Äôve also enjoyed using jsonmodels for writing super-simple JSON data models, and I‚Äôm using it to write a basic data model for digital objects and files in archival description. Maybe the filesystem specifications could be based on the Oxford Common File Layout (OCFL). Here‚Äôs a quick-and-dirty example of vaguely what I‚Äôm thinking based on Bagit: &amp;lt;digital object&amp;gt;/ | -- metadata.yml | -- bag-info.txt | -- manifest-&amp;lt;algorithm&amp;gt;.txt | -- tagmanifest-&amp;lt;algorithm&amp;gt;.txt | +-- files/ | +-- &amp;lt;file&amp;gt;/ | -- content.txt | -- metadata.yml | -- thumbnail.jpg | +-- versions/ | +-- &amp;lt;version&amp;gt;/ | | -- metadata.yml | | -- &amp;lt;access file&amp;gt;.jpg | +-- &amp;lt;version&amp;gt;/ | -- metadata.yml | -- &amp;lt;original file&amp;gt;.tif +-- &amp;lt;file&amp;gt;/ ... Essentially, this data controls what gets sent to Solr, which controls what is displayed and made accessible in Blacklight and Arclight. Which means librarians and archivists can control our access systems by copying and pasting files and editing text files and spreadsheets. We should start by ensuring that this work can be done manually, and then slowly build as-simple-as-possible tools and utilities to automate labor-intensive tasks. Maybe these are command line scripts, maybe they‚Äôre super simple Electron or Flask apps. Jobson looks cool. Need a web form to create objects? That could be an optional single page app. Using common tools like filesystems and spreadsheets that people use everyday outside of our profession makes it more likely that we could use or adapt a variety of workflow tools that may be developed for different purposes. This would ensure that our practices are extensible from small repositories that can still do the work manually to large institutions working at scale. This also puts us all on the same foundation making it more likely that tools and processes developed at larger repositories could actually benefit smaller archives. This model also fits regional consortiums that support small repositories well. We can index from a common layout in Google Drive! That could be a bit clunky, but feasible! Consortia can better host common large applications like Solr, Blacklight/Arclight, and image servers while local institutions manage their own data. If the data is structured and managed well enough according to a common specification, consortia can read (almost crawl?) and index the data from a couple different sources, including commercial cloud storage. This allows them to meet small archives wherever they are at. The reverse could also work too, as medium-sized archives could use tools or scripts to read their data and POST it into a consortia Solr. What I also like about this is that you also don‚Äôt need to be an instant expert to participate. When I started learning how to do this work, I was writing XML by hand. Now the grad students I work with only see the shiny web forms in ArchivesSpace or Hyrax. It‚Äôs has become so much harder to onboard new professionals and have them contribute to access in a meaningful way. A manual-first access system facilitates step-by-step learning where we can all start from the same foundation and adopt individual tools one-by-one over time. There are still major challenges here. Quality specifications and data models take a lot of community consensus-building. Validation is a concern, although we can build simple tools that allow librarians and archivists to validate and evaluate their own data. I also haven‚Äôt fully wrapped my head around how versioning might work in a way that accommodates my practical local examples, what OCFL is doing, and the spirit of OAIS. How would we get access files to an image server? From a preservation risk perspective, it‚Äôs also easy to delete a lot manually. But read-only filesystems exist, and we should be able to design systems with substantial fault tolerance. I‚Äôd also argue that separating file and data management from complex monolithic tools also eliminates significant preservation risk. There are problems than would need to be addressed, but these are types of challenges that librarians and archivists are well equipped to take on in collaboration with technologists. Overall, I‚Äôm really intrigued with the design scope of Blacklight and Arclight because they make it possible for librarians, archivists, and technologists with a wide variety of expertise and experiences to design and build access systems together. If we use common, everyday tools like filesystems, text files, spreadsheets, network file shares, and commercial cloud storage for file and data management, librarians and archivists can take on this work, allowing them to be equal partners with technologists in building access systems for digital materials. I think this would also better match this work with our organizational structures, as currently system design is often either too concentrated in technology departments, or design is done in practice by library leadership selecting vendors. I also think most libraries don‚Äôt have a good process of evaluating vendor services and base much of this on reputation and branding.2 Separating data management from digital repositories and using common tools could allow even smaller libraries and archives to retain full control of their data. There can be roles for vendors, but the path I am envisioning relegates them to individual tools. Hosting Solr or the front-end? Sure. A vendor OCR or transcription service? Sign me up. I‚Äôm sure vendors will fill in gaps and help with particularly painful processes. Designing access systems around filesystems and spreadsheets requires strong specifications and data models, which pushes this design work in small and medium-sized archives away from administrators (who have different skillsets) to practitioners‚Äîlibrarians, archivists, and technologists‚Äîwho work directly with materials and core services. This work can then be done collaboratively and collectively, with everyone contributing their expertise. Let‚Äôs match our systems to our organizational strengths. Notes If you have some coding experience and want to try out Blacklight first-hand, this tutorial was really helpful for me.¬†&amp;#8617; I feel like there‚Äôs this phenomenon in libraries where the technologists have too much power in how our systems and services work, so librarians and archivists in turn marginalize technologists in planning and policymaking. Now, I have heard about places where technologists were put in charge of a special collections department or academic library, which might be the darkest timeline, so some of this might be rational defense, but there‚Äôs also definitely a role for technologists in planning and policymaking!¬†&amp;#8617;</summary></entry><entry><title type="html">Lost Without Context: Representing Relationships between Archival Materials in the Digital Environment</title><link href="http://localhost:4000/2021/11/01/lost-without-context.html" rel="alternate" type="text/html" title="Lost Without Context: Representing Relationships between Archival Materials in the Digital Environment" /><published>2021-11-01T00:00:00-04:00</published><updated>2021-11-01T00:00:00-04:00</updated><id>http://localhost:4000/2021/11/01/lost-without-context</id><content type="html" xml:base="http://localhost:4000/2021/11/01/lost-without-context.html">&lt;p&gt;This is the group contribution I facilitated for the &lt;a href=&quot;https://exhibits.stanford.edu/lightingtheway/feature/working-meeting-spring-2021&quot;&gt;Lighting the Way Working Meeting&lt;/a&gt;, published in Matienzo, M.A. &amp;amp; Handel, Dinah, eds. &lt;a href=&quot;https://purl.stanford.edu/gg453cv6438&quot;&gt;The Lighting the Way Handbook: Case Studies, Guidelines, and Emergent Futures for Archival Discovery and Delivery&lt;/a&gt; (November 2021).&lt;/p&gt;

&lt;p&gt;The rest of the authors deserve credit for this article. I organized and facilitated our working meeting sessions and did some editing and small contributions to the piece.&lt;/p&gt;

&lt;p&gt;Abstract:&lt;/p&gt;

&lt;p&gt;The problem of representing context for archival materials in digital asset management
systems (DAMS) has been noted - and lamented - for as long as digital representations of archives have
been online. This white paper discusses the nature of this challenge, explores why it remains so thorny,
and provides examples of where archival access systems have been successful in representing context.
With hopes of moving the conversation forward, we provide a set of principles for representing archives
in context that can be implemented regardless of the particular systems employed. These principles
are based on archival standards and software best practices, and can be summarized as six ideas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create space for deep conversations with all stakeholders and so that everyone understands
foundational requirements.&lt;/li&gt;
  &lt;li&gt;Value archival context and design systems so that contextual relationships between records are
explicit and clear.&lt;/li&gt;
  &lt;li&gt;Leverage the power (and cost savings) of aggregate digitization and description when
appropriate.&lt;/li&gt;
  &lt;li&gt;Be consistent about modelling relationships between an analog object (if relevant), a digital
object, and the description of the archival record.&lt;/li&gt;
  &lt;li&gt;Use persistent identifiers.&lt;/li&gt;
  &lt;li&gt;Lean on widely-used standards, systems, and solutions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, we call on standards-making bodies to introduce a more robust data model for archival
representation that includes both the description of archival contents and contexts.&lt;/p&gt;</content><author><name>Jodi Allison-Bunnell, Maureen Cresci Callahan, Gretchen Gueguen, John Kunze, Krystyna K. Matusiak, and Gregory Wiedeman</name></author><summary type="html">This is the group contribution I facilitated for the Lighting the Way Working Meeting, published in Matienzo, M.A. &amp;amp; Handel, Dinah, eds. The Lighting the Way Handbook: Case Studies, Guidelines, and Emergent Futures for Archival Discovery and Delivery (November 2021). The rest of the authors deserve credit for this article. I organized and facilitated our working meeting sessions and did some editing and small contributions to the piece. Abstract: The problem of representing context for archival materials in digital asset management systems (DAMS) has been noted - and lamented - for as long as digital representations of archives have been online. This white paper discusses the nature of this challenge, explores why it remains so thorny, and provides examples of where archival access systems have been successful in representing context. With hopes of moving the conversation forward, we provide a set of principles for representing archives in context that can be implemented regardless of the particular systems employed. These principles are based on archival standards and software best practices, and can be summarized as six ideas: Create space for deep conversations with all stakeholders and so that everyone understands foundational requirements. Value archival context and design systems so that contextual relationships between records are explicit and clear. Leverage the power (and cost savings) of aggregate digitization and description when appropriate. Be consistent about modelling relationships between an analog object (if relevant), a digital object, and the description of the archival record. Use persistent identifiers. Lean on widely-used standards, systems, and solutions. Finally, we call on standards-making bodies to introduce a more robust data model for archival representation that includes both the description of archival contents and contexts.</summary></entry><entry><title type="html">Review of Archival Values: Essays in Honor of Mark A. Greene</title><link href="http://localhost:4000/2020/12/04/review-archival-values.html" rel="alternate" type="text/html" title="Review of Archival Values: Essays in Honor of Mark A. Greene" /><published>2020-12-04T00:00:00-05:00</published><updated>2020-12-04T00:00:00-05:00</updated><id>http://localhost:4000/2020/12/04/review-archival-values</id><content type="html" xml:base="http://localhost:4000/2020/12/04/review-archival-values.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">What‚Äôs Your Set-up?: Processing Digital Records at UAlbany</title><link href="http://localhost:4000/2020/04/21/whats-your-set-up.html" rel="alternate" type="text/html" title="What‚Äôs Your Set-up?: Processing Digital Records at UAlbany" /><published>2020-04-21T00:00:00-04:00</published><updated>2020-04-21T00:00:00-04:00</updated><id>http://localhost:4000/2020/04/21/whats-your-set-up</id><content type="html" xml:base="http://localhost:4000/2020/04/21/whats-your-set-up.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">The Historical Hazards of Finding Aids</title><link href="http://localhost:4000/2019/12/20/historical-hazards.html" rel="alternate" type="text/html" title="The Historical Hazards of Finding Aids " /><published>2019-12-20T00:00:00-05:00</published><updated>2019-12-20T00:00:00-05:00</updated><id>http://localhost:4000/2019/12/20/historical-hazards</id><content type="html" xml:base="http://localhost:4000/2019/12/20/historical-hazards.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">Describing Web Archives: A Computer-Assisted Approach</title><link href="http://localhost:4000/2019/12/09/describing-web-archives.html" rel="alternate" type="text/html" title="Describing Web Archives: A Computer-Assisted Approach" /><published>2019-12-09T00:00:00-05:00</published><updated>2019-12-09T00:00:00-05:00</updated><id>http://localhost:4000/2019/12/09/describing-web-archives</id><content type="html" xml:base="http://localhost:4000/2019/12/09/describing-web-archives.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">UAlbany ArcLight Update and Outlook</title><link href="http://localhost:4000/2018/11/19/arclight-update.html" rel="alternate" type="text/html" title="UAlbany ArcLight Update and Outlook" /><published>2018-11-19T00:00:00-05:00</published><updated>2018-11-19T00:00:00-05:00</updated><id>http://localhost:4000/2018/11/19/arclight-update</id><content type="html" xml:base="http://localhost:4000/2018/11/19/arclight-update.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry><entry><title type="html">Migrating to ArchivesSpace at UAlbany</title><link href="http://localhost:4000/2017/04/01/migrating-to-archivesspace.html" rel="alternate" type="text/html" title="Migrating to ArchivesSpace at UAlbany" /><published>2017-04-01T00:00:00-04:00</published><updated>2017-04-01T00:00:00-04:00</updated><id>http://localhost:4000/2017/04/01/migrating-to-archivesspace</id><content type="html" xml:base="http://localhost:4000/2017/04/01/migrating-to-archivesspace.html">&lt;p&gt;This is an overview our ArchivesSpace migration process and descriptions of the code used. Hopefully this rather long-winded description helps document our processes and thinking.&lt;/p&gt;

&lt;p&gt;Overall, my strategy was to import everything twice. Once we got most of our data into ASpace, I let our staff have a free run of the data to get comfortable with the application. We maintained our previous data stores as masters, and staff could create, edit, and delete ASpace data as much as they wanted. When we were done testing and I was comfortable with all the migration scripts, we cleared out all the data and made a clean import.&lt;/p&gt;

&lt;h2 id=&quot;clean-up-ead-files-for-the-archivesspace-importer&quot;&gt;Clean-up EAD files for the ArchivesSpace importer&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;importFix.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step one was getting all of our EAD data reformatted so they would all easily import into ASpace. I wrote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;importFix.py&lt;/code&gt; to make duplicate copies of all of our EAD XML files while stripping out and cleaning any issues that I found would cause errors during import. An example issue was that ASpace didn‚Äôt want extent units stored in an attribute like 
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;extent unit=&quot;cubic ft.&quot;&amp;gt;21&amp;lt;/extent&amp;gt;&lt;/code&gt; Instead, it wanted &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;extent&amp;gt;21 cubic ft.&amp;lt;/extent&amp;gt;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We had been using an internal semantic ID system, but we wanted to get rid of these and rely on the ASpace ref_IDs instead, so the script also removes all the @id attributes in &lt;c01&gt;, &lt;c02&gt;, etc.&lt;/c02&gt;&lt;/c01&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can also fix these issues with a plugin that customized how ASpace imports the files. Alex Duryee has spoken on this at &lt;a href=&quot;https://github.com/alexduryee/saa-aspace-demo&quot;&gt;SAA&lt;/a&gt; and &lt;a href=&quot;https://github.com/alexduryee/beyondthebasics&quot;&gt;elsewhere&lt;/a&gt;. I was already much more comfortable with Python and lxml, so this was faster for a one-off import process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While we had employed a lot of rule-based validation to ensure standardization. There were other issues, like ASpace understood that 1988-03-30/1981-01-01 was an invalid date, but our validation script did not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the script also identified components with empty &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;unittitle&amp;gt;&lt;/code&gt; tags, etc., so I could get a list of these issues prior to import. Once I had identified the most common problems, this was much easier than having background jobs keep failing halfway through.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This was a tedious process with lots of trial and error that took almost 2 complete days. I had to re-import a bunch of times, so I wrote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deleteRepoRecords.py&lt;/code&gt; to delete all resource records in ASpace (obviously use cautiously).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The feedback from ASpace on import issues can be really frustrating. It does not give you feedback on the EAD file that was the culprit, but points you to the new native ASpace record that was never actually created. You have to dig a bit in the log to find the EAD filename. Since I has stripped out our IDs, I was only left with the ASpace ref_id which is not helpful at all in finding individual components among our EAD jungle, so I had to rely on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;unittitle&amp;gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;unitdate&amp;gt;&lt;/code&gt; strings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You‚Äôll see feedback like, ‚Äúthere‚Äôs an issue in note[7].‚Äù It takes a while to figure out that note[7] means &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;physdesc&amp;gt;&lt;/code&gt;, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We only have one real import incompatibility with  ASpace, and that was only one EAD file where 7,800 lines of the 13,000 line file was one gigantic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;chronlist&amp;gt;&lt;/code&gt;. ASpace rightly denied this garbage, so we just pulled it out of the finding aid and maybe it will become a part of a web exhibit some day.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We still had a some messy data when you checked the controlled values lists, but these were few enough that it was easier to identify and clean these issues after the migration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the end of this process, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;importFix.py&lt;/code&gt; allowed us to keep creating and maintaining our EAD files in our current systems, but also let me quickly create ASpace-friendly versions when I need to.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dont-repeat-yourself&quot;&gt;Don‚Äôt Repeat Yourself&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/UAlbanyArchives/archives_tools&quot;&gt;aspace Python Library&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When I wrote the delete resources script I discovered that working with the API is awesome, but I was also repeatedly calling the same API requests. There is no complete Python library for the ArchivesSpace API, so most people make calls manually with &lt;a href=&quot;http://docs.python-requests.org/en/master/&quot;&gt;requests&lt;/a&gt;. Artefactual labs also has a &lt;a href=&quot;https://github.com/artefactual-labs/agentarchives&quot;&gt;basic library&lt;/a&gt; for this, but it doesn‚Äôt do a lot for you and still involves manually dealing with a lot of JSON in a way that I didn‚Äôt find intuitive. Basically, I wanted to iterate though the ASpace data as easily as lxml loops though XML data. This often involves multiple API calls for a single function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the course of the migration I started writing a bunch of functions that manage the API behind the scenes. This was I can quickly iterate though collections with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getResources()&lt;/code&gt;, collection arrangement with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getTree()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getChildren()&lt;/code&gt; and even find a collection by its identifier with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getResourceID()&lt;/code&gt;, or a location by its title string with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;findLocations()&lt;/code&gt;. Its also handy to have some blank objects at your fingertips, like with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;makeArchObj()&lt;/code&gt;, so you can create new archival objects without knowing exactly what JSON ASpace needs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There‚Äôs also some debugging tools I use all time. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pp()&lt;/code&gt; pretty prints an object to the console as JSON, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fields()&lt;/code&gt; lists all the keys in an object, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;serializeOutput()&lt;/code&gt; writes objects to your filesystem as .json files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The library uses &lt;a href=&quot;https://pypi.python.org/pypi/easydict/&quot;&gt;easydict&lt;/a&gt; to make objects out of the JSON, so you can call it with dot notation (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print collection.title&lt;/code&gt;). Writing this library enabled me from rewriting the same code, or finding that script I used last week to steal some lines from. Its really cool to be able to do this:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  from archives_tools import aspace as AS
    
  session = AS.getSession()
  repo = &quot;2&quot;
    
  for collection in AS.getResources(session, repo, &quot;all&quot;):
  	if collection.title = &quot;David Baldus Papers&quot;:
  		collection = AS.makeDate(collection, 1964-11, 2001-03)
  		post = AS.postResource(session, repo, collection)
  		print (post)
    		
  &amp;gt; 200
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some caveats here. There are a bunch of decisions you make while writing these that I kind of made arbitrarily. It‚Äôs probably more complex than it has to be. Ideally, this would be more a a communally-driven effort based on a wider set of needs and perspectives. The code is completely open, so if nothing else, I guess this could provide an example of what a Python library for ASpace might look like. I‚Äôm going to try and complete the documentation soon, and if anyone has suggestions I‚Äôd really like to hear them. If the community moves to a different approach, I‚Äôll work to update all of our processes to match the consensus, since it will be better than what I can come up with on my own.&lt;/p&gt;

&lt;h2 id=&quot;import-collection-level-records&quot;&gt;Import collection-level records&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateCollections.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Before ArchivesSpace, we used a custom ‚ÄúCMS‚Äù system for collection management and accessioning. We think it‚Äôs over 15 years old, and the only way to get data out is directly from the database tables which looked like they were copied from some other system and were overtly complicated and often repeated data. Moreover, the only way to view entire records was to go into edit mode where you could easily overwrite data. If you picked an accession number for a new accession that was already taken, it would just override the old record (!!!). Overall it was a pain to enter information in the system and over time the department slowly stopped updating all but the most necessary fields. Yet, this system had the only total listing of all collections.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Over the past year, we found that as we got more and more of our archival description into EAD, we needed an automated way to push this data into our Drupal-driven website, so I pulled a master collection list out of this collection management system, updated and fixed it and just dumped it into a spreadsheet. For each collection it had a normalized name, listed if there was an EAD or HTML finding aid, and had an extent, inclusive date, and abstract for everything not in EAD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While we‚Äôve been making great strides in getting collection-level records for everything, almost 1/3 on our collections sill only had accession records in the old system, and a row in this spreadsheet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This spreadsheet has been our master collection list and a key part of the backend of &lt;a href=&quot;http://library.albany.edu/archive&quot;&gt;our public access system&lt;/a&gt; (not recommended). ArchivesSpace now allows us to replace this setup by pulling all of this information directly from the API.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yet, with this spreadsheet I could use &lt;a href=&quot;https://openpyxl.readthedocs.io/en/default/&quot;&gt;a Python library for .xslx files&lt;/a&gt; to import some basic resource records into ASpace. While they wouldn‚Äôt be DACS-compliant, we could still export this basic data to our public access system, and link them to accession records we‚Äôll import later.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;migrateCollections.py&lt;/code&gt; iterates though the spreadsheet and uses the aspace library to post new resource records in ASpace.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;migrating-accessions&quot;&gt;Migrating Accessions&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateAccessions.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For accessions I asked the Library‚Äôs awesome database person Krishna for help. Our old ‚ÄúCMS‚Äù was a .asp web application which, from what I can tell, used an old SQL server for its database. I needed something that I could loop though with Python so I asked him to export some CSVs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once he made the first export we noticed some problems. First there were a lot of commas in some of the fields, so we had to use a different delimiter. The other problem was there was a bunch of carriage returns that use the same character as the csv new line, so we had to escape all these prior to the export. I tried fixing it with a Python script by counting the number of fields in each row and piecing it back together humpty-dumpty-style, but I never want to do something that frustrating again. Luckily with a little time, Krishna was able to escape them before export and use pipes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt;) as the delimiter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once I was able to loop through the table with all the accessions, I met with Jodi and Melissa, some of our other archivists, to identify which fields were still held useful data and how to map them to ASpace accession records. Honestly, we threw out half the data because it was useless, but we were able to reclaim all the accession dates, descriptions, and extents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We had a handful of issues that had to be edited manually. Even though the system returned an error if you forgot to enter an accession number, there were a bunch of records without numbers or records without numbers. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkAccessions.py&lt;/code&gt; identified these issues, but we had to go look at each record and try to parse out what happened. The good thing was the data did include a last updated timestamp, which helped a lot.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Of course the first time I tried this I opened the CSV file in Excel it unhelpfully decided to reformat all of the timestamps and drop all the trailing zeros. Good thing I kept a backup file. Since it was only a handful I ended up doing the edits in a text editor, just to be safe.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The cool thing was that the collection IDs were really standardized, so we could easily match them up with the resource records we just created. We just had to convert APAP-312 to apap312, which is a piece of cake with Python‚Äôs string methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The aspace library really came in handy here. Two functions, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;makeExtent()&lt;/code&gt; and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;makeDate()&lt;/code&gt; just add extents and dates to any ASpace JSON object. This worked with accessions just as it did with resources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The extent was just an uncontrolled string field. Considering this, we had used the field very consistently, but of course there were a bunch of typos or oddly worded unit descriptions. I cleaned up the vast bulk of these issues with Python‚Äôs string methods, but there were still some ‚Äúlong-tail‚Äù issues that didn‚Äôt seem to be worth spending hours on. After all, we haven‚Äôt needed real machine-readable accession extents yet, and its not publicly-visible data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For now I just allowed a new extent type ‚Äúuncontrolled,‚Äù and threw the whole field in the number field. We can just fix these one by one as we come across them. I guess I could have dumped the CSV into OpenRefine, but I can also come back later and export a new CSV with the API if we need to clean these up further.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;migrateAccessions.py&lt;/code&gt;, we had accession records and collection descriptions in the same place for the first time. I was really wonderful seeing those Related Accessions and Related Resources links pop up in ASpace.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;locations&quot;&gt;Locations&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	migrateLocations.py
	exportContainers.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Locations turned out to be the trickiest part of the migration, because our local practices really conflicted with how ASpace manages locations. Theory-wise ASpace does this much better and eventually our practices will improve after the migration, but I had to do some hacky stuff to get ASpace to work with our locations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Basically, in ASpace 1.5+ containers are separated out and managed separately from archival description and connected to both location and description records with URI link. This is great because it lets us describe records independently from how they are managed physically. The newer ASpace versions also let you do some really cool things like calculate extents if you have good data and also supports barcoding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In ASpace collections don‚Äôt have locations. How can they, since archival description is really an intangible layer of abstraction that just refers to boxes and folders? In ASpace, collections (or any other description records) are linked to Top Containers (boxes) which are linked to locations. Long-term this is great, since it lets us manage the intellectual content of a collection differently from its physical content.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yet, this poses a very practical problem for us. Our old ‚ÄúCMS‚Äù had only one location text field per collection, so we really only have collection-level locations. We‚Äôve never really had a problem with our location records because we have really awesome on-site storage. It also helps that most of our staff has been here a long time, and at least generally knows where most collections are anyway.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The location field we had was really messy. This is one extreme real-world example:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  H-16-3-1/H-16-2-3,(last shelf has microfilm and CDs with images used for LUNA); 
  G-8-1-1/G-8-2-1; G-17-3-2/G-17-3-1; G-17-4-1/G-17-4-7; G-10-4-1/G-10-4-3;
  G-10-5-1/G-10-5-8;C-12-2-1/C-12-2-5; C-12-1-2/-12-1-3; SB 17 - o-15 (bound
  copies of Civil Service Leader digitized by Hudson Micro and returned in 
  2013, bound and unbound copies of The Public Sector, 1978-1998, digitized
  and returned in 2015, unbound copies of The Work Force, 1998 through 2012, 
  also digitized); Cold 1-1 - 1-4, E-1-1, A-1-7 - A-1-8, A-1-5 - A-1-7,
  A-1-9, A-1-8, A-1-8 - A-1-9, A-5-1 - A-5-2, A-5-2 - A-5-3, A-4-9 - A-5-1,
  A-6-5, A-5-5, A-6-4 - A-6-5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So that collection has a bunch of top containers, how do I assign that mess to them? Well this collection in particular is currently being, well, &lt;em&gt;reworked&lt;/em&gt;, but for everything else we had to standardize the data into one format, basically:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  G-4-2-3/G-4-2-7 (whatever note here); J-3-3-5 (second location here); I-7-3-6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then one of our awesome graduate students, Ben Covell, learned &lt;a href=&quot;http://openrefine.org/&quot;&gt;OpenRefine&lt;/a&gt; in about a hour and cleaned up all the data in less than a day. (Part of the solution always seems to be having great grad students)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, we decided it wasn‚Äôt worth it to go through every box in the archives to get individual shelf coordinates. Thus, we had two options, either use a separate system for shelf lists, or hack ASpace to make it work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I tried and failed to get ASpace to accept multiple locations per box, which seems like it really shouldn‚Äôt be allowed. You can actually have a top container linked to multiple shelves, but it won‚Äôt let you then link those boxes to description records.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASpace does let you have previous locations though, but it requires an end date, and the location record to have a temporary label. We decided to have a temporary ‚Äúcollection_level‚Äù label and setting the end date to 2999-01-01, and hope that we weren‚Äôt creating an insurmountable Y3K problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Doing it this way meant a lot of work with the API, because we had to find every top container assigned to each collection, translate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;G-17-3-8&lt;/code&gt; to a ASpace location record, find that record, update that location record to be temporary, and update the top_container record to add each location. This is exactly what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;migrateLocations.py&lt;/code&gt; does, and with the library it only takes 200 lines of code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So that means if a collection-level location was &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C-24-2-1/C-24-3-8&lt;/code&gt; (16 shelves), every box in that collection (probably about 40-50 boxes) would be linked to each of those 16 locations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For larger collections, this would be a bit unwieldy, so the plan is to list individual shelves for larger collections now, and do the rest over time. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exportContainers.py&lt;/code&gt; helps with this. It makes a .xlsx spreadsheet for each collection that lists every top container and its URI. So we have a directory of spreadsheet files for each collection like apap101.xlsx. We can list the shelf coordinates for each box in that spreadsheet in our same format (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K-2-2-3&lt;/code&gt;) and use the API to update each top container record over time. Now whenever we‚Äôre working with a collection we can fix this problem one by one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, ASpace comes with some really cool features like container profiles and shelf profiles, but they‚Äôre not mandatory. We decided it wasn‚Äôt worth it yet to measure each shelf, but we hope to do this over time in the next couple of years.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, migrating locations ‚Äúthe right way‚Äù would have taken months and months of effort and involved every staff member in the department. Instead, I was able to basically do it myself with a little help in about a week. We might run into a problem if updates to ASpace changes how the application handles these records, but even in that worst-case scenario we can just export all our incompatible locations with the API before upgrading and use a separate shelf list system. Our data will even be a lot better too.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learn-by-breaking-things&quot;&gt;Learn by Breaking Things&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We really didn‚Äôt have the option to bring in Lyrasis to do workshops for our staff on how to use ASpace. This is going to be the primary medium for our department to create metadata and make our holdings available, so they need to be comfortable with it, and switching everyone over to a whole new system without any formal prep is a lot to ask.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best way I find to learn a tool is to get some hands-on experience and try and break it to see where the boundaries are. Migrating all our data twice, not only lowered the stakes for the first import, but it gave us a lot of time with a fully-running ArchivesSpace with a lot of realistic test data. I opened it up to the our department and encouraged them to make new resources and accessions, delete things, and generally try test their boundaries and get comfortable. I pointed them to &lt;a href=&quot;http://www2.archivists.org/sites/all/files/OK%20State%20U%20Using%20ArchivesSpace.pdf&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;http://www2.archivists.org/sites/all/files/UNO_ArchivesSpace_Walkthroughs_2015.pdf&quot;&gt;guides&lt;/a&gt; shared by the SAA Collection Management Tools Section, and they also said there‚Äôs a bunch of &lt;a href=&quot;https://www.youtube.com/results?search_query=archviesspace&quot;&gt;screencasts on YouTube&lt;/a&gt; that really helped.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, ASpace is very intuitive, and easy to use if you comfortable working with web applications. Our department has taken to it fairly quickly, and I don‚Äôt think more formal training would of helped much. Having a really good sandbox and dedicating real time to experimenting with it is definitely key.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We‚Äôre also still working on establishing best practices for use and documentation for our students. I think this is actually a bigger hurdle than leaning the tool itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;building-satellite-tools&quot;&gt;Building Satellite Tools&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The best reason for implementing ArchivesSpace isn‚Äôt necessarily its basic functionally of creating and managing archival description. ASpace‚Äôs open architecture means that essentially all the data in the system can be read or updated with the API. This is not only for automating changes, but it also allows us to use other tools to interact with our data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The other really cool thing is that the ASpace API is backed by a data model. This way it closely defines its acceptable data, and rejects anything outside of its parameters. I‚Äôve found that this is makes life so much easier than say, managing your data in XML, since it has the effect of making your data much more stable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If ASpace doesn‚Äôt do something the way you like, you can just use a different tool to interact with your ASpace data. This way, ASpace can really support workflows that embrace the separation-of-concerns principle. Here ASpace is at the center of an open archives ecosystem, and you can have smaller, impermanent tools to fill specific functions really well. Often, a modular system like this can be easier to maintain, as in the future you can replace these smaller tools one-by-one as they become obsolete, rather than the daunting task of one huge migration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A lot of this is built upon the work Hillel Arnold and Patrick Galligan have done at the Rockefeller Archive Center. They wrote a &lt;a href=&quot;https://github.com/RockefellerArchiveCenter/as-cors&quot;&gt;plugin&lt;/a&gt; that lets you work with the API from a &lt;a href=&quot;http://blog.rockarch.org/?p=1610&quot;&gt;single HTML page with JavaScript&lt;/a&gt;. They used this to write &lt;a href=&quot;https://github.com/RockefellerArchiveCenter/find-it&quot;&gt;Find-it&lt;/a&gt;, a &lt;a href=&quot;http://blog.rockarch.org/?p=1621&quot;&gt;simple text field to return locations for ASpace ref_ids&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find-It solved a key problem for us as one of the only things our old ‚ÄúCMS‚Äù did really well was that you could type in ‚ÄúCrone‚Äù and quickly get shelf locations for the Michelle Crone Papers. Our other archivists were asking for something similar in ArchivesSpace, and didn‚Äôt like they you had to look through three different records to get this information in ASpace. Yet, the odd way I did our locations posed some problems. Ref_ids would also work, but we needed to return locations for top containers assigned to resources as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With my hacky JavaScript skills (and some help from Slack) I set it up to also return locations for resources using the search API endpoint and the id_0 field. Using the search API made me realize that I could also make an API call that returns a keyword search for resources. The search returns links to the Resource page in ASpace and XTF as well, which makes Find-it a really quick first access point to our collections as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The last request I got was for some way to access locations from resources with lower levels of description, so I was even able to call the resource tree and add links to lower levels if there are no instances. I just made find-it accept ids as a hash and these links just reloads the page with that lower-level ref_id. Overall it became a bit more complex than I was envisioning. Our fork is &lt;a href=&quot;https://github.com/UAlbanyArchives/find-it&quot;&gt;here&lt;/a&gt;, but I‚Äôd recommend starting with Rockefeller‚Äôs much cleaner version and using ours as more of an example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, the single search bar on a white background was really boring, so I wrote a quick script that updates the background to &lt;a href=&quot;http://www.bing.com/gallery/&quot;&gt;Bing‚Äôs image of the day&lt;/a&gt; &lt;a href=&quot;http://stackoverflow.com/questions/10639914/is-there-a-way-to-get-bings-photo-of-the-day&quot;&gt;API&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/slides/img/findit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Another workflow issue we found was how tedious it could be to enter in individual containers for large accessions. Say we got a new collection of 50 boxes. ASpace makes you create each of these boxes, which is good, but very tedious. So I basically used the same framework as Find-it, figure our how to get ASpace to &lt;a href=&quot;https://github.com/UAlbanyArchives/as-cors&quot;&gt;accept Cross-Origin post requests&lt;/a&gt;, and made a single-page box uploader. Now we can just list new boxes in these fields with our old-style shelf coordinates, and a bunch of JavaScript will create and assign all these new top containers to a resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/slides/img/boxes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The last request I got from out staff was for creating container list inventories. We use undergraduate student assistants to do most of this data entry work, and we found that ASpace rapid data entry would require a good amount of training for our students who work few hours with a really high turnover rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So I wrote &lt;a href=&quot;https://github.com/UAlbanyArchives/asInventory&quot;&gt;some Python scripts&lt;/a&gt; to manage inventory listings with spreadsheets and the ASpace API. One parses a .xslx file and uploads file-level archival objects through the API. It will also take a bunch of access files and makes digital objects, places the files on our server and links them to archival objects. Another script reads an existing inventory back to a spreadsheet with all the relevant URIs so we can roundtrip these inventories for future updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I also envision this as a near-term solution, as its possible that we end up doing all this natively in ASpace in the future. The cool thing is that the ASpace API makes all these tools really easy and quick to make, and they solve real immediate problems for us. Yet, none of these tools are really essential for us, and if they become too difficult to manage, we can just drop them and move on to something else.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;migration-days&quot;&gt;Migration Day(s)&lt;/h2&gt;

&lt;h3 id=&quot;ead-import&quot;&gt;EAD import&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;importFix.py
checkEAD.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When migration day came, we dumped the database, deleted the index, and started from scratch. I had Krishna export new CSVs from our old system with any updated records that were made since the previous export. By this point the plan was that migration scripts were done and tested and everything would go relatively seamlessly. I was hoping to get everything imported in a day or so, and minimize the downtime when staff were not allowed to make updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When it came time for the EAD import, some recent EADs/updated had happened, and I still got 2 minor errors for about 630 EADs, so I didn‚Äôt actually get the Holy Grail of a perfect EAD import. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1963-0923&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1904-01/1903-07&lt;/code&gt; are not valid dates and ArchivesSpace, but these were easy fixes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One thing I had noticed during the first time was than when you import a large amount of EADs, it takes the index awhile to catch up. ASpace still told me I had only 465 resources instead of 633, but that soon updated to 503. Yet, in about an hour I was still missing 29 collections. I assumed that this was still the index, but I quickly wrote checkEAD.py to see how many collections I could access though the API and I was still missing 29.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It turned out that two import jobs had actually failed, but ASpace listed them as Complete for some reason. One issue was when we changed an ID and there was an eadid conflict, and the other issue was that I totally forgot about that huge &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;chronlist&amp;gt;&lt;/code&gt; that failed the first time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These were easy fixes but the lesson here is always test your imports and don‚Äôt trust the index.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The importing ended up taking most of the afternoon, so I discovered  my one-day migration plan was over-optimistic.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;clean-resource-identifiers&quot;&gt;Clean Resource Identifiers&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fixNam.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For collection identifiers we had been unsing the prefix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nam_&lt;/code&gt; which is our OCLC repository ID or something, but we decided that this was overtly complex and not useful, so we decided to drop these late in the migration planning. While the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;importFix.py&lt;/code&gt; script was written to clear the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nam_&lt;/code&gt; prefix from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ead&amp;gt; @id&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;eadid&amp;gt;&lt;/code&gt;, but I forgot to do this to the collection-level &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;unitid&amp;gt;&lt;/code&gt;. Unfortunately I found out the hard way that this is what ASpace ends up using as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id_0&lt;/code&gt;, so I whipped up &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fixNam.py&lt;/code&gt; to strip this post-import rather than start the process over again.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;collection-level-spreadsheet-import&quot;&gt;Collection-level spreadsheet import&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateCollections.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Importing the basic collection-level records from the spreadsheet went fairly well. I realized that we also needed a normalized name for each collection in the resource records somewhere. For example, the ‚ÄúOffice of Career and Professional Development Records‚Äù needed to be accessible somewhere as ‚ÄúCareer and Professional Development, Office of; Records‚Äù so it would be alphabetized correctly in our front end. So, I added a few lines in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;migrateCollections.py&lt;/code&gt; that adds these normal names in the Finding Aid title field.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One annoying thing was that after I posted all the spreadsheet records, the index was a bit slow to update. I was all set to migrate the accessions, but to match them up with the resource records, I had to use the index to search for matching id_0 fields. However, within an hour or so suddenly ASpace had updated the index, and I was all set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASpace told me I had 904 resources, which was curiously two more than we had in our master list. I assumed we failed to update our master list for two EADs and I imported them from the spreadsheet as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To confirm all the collections, I wrote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkCollections.py&lt;/code&gt; to see what was up. It turned out that when we had updated that ID we still had a duplicate EAD file floating around, and another recent collection had an EAD file but was never entered in to the spreadsheet. These issues were easily fixed and all our collections were done.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;accessions-and-locations&quot;&gt;Accessions and Locations&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;migrateAccesions.py
checkAccessions.py
migrateLocations.py
checkLocations.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The accessions and locations went easy without any issues. This time I wrote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkAccessions.py&lt;/code&gt; to find all the accession issues from last time around, and everything went nicely.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some of the locations had been updated since the last import, so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkLocations.py&lt;/code&gt; verifies that these locations are indeed in ArchivesSpace, prior to running the import scripts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;export-scripts&quot;&gt;Export Scripts&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exportPublicData.py
lastExport.txt
exportConverter.py
staticPages.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Once we had migrated all of our data into ArchivesSpace, it was time to set up scheduled exports and connect everything with our public access system that consists of XTF and a sets of static pages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rockefeller‚Äôs &lt;a href=&quot;https://github.com/RockefellerArchiveCenter/asExportIncremental&quot;&gt;asExportIncremental.py&lt;/a&gt; is a good source for how to do export EADs from ASpace incrementally. I followed the same basic idea, but omitted the MODS exports, and simplified it a bit into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exportPublicData.py&lt;/code&gt;. I integrated the same API call into the &lt;a href=&quot;https://github.com/UAlbanyArchives/archives_tools&quot;&gt;aspace library&lt;/a&gt; as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getResourcesSince()&lt;/code&gt; which takes a POSIX timestamp. Instead of using Pickle I just wrote the last export time to a text file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lastExport.txt&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASpace‚Äôs EAD exports didn‚Äôt exactly match up with what we had been using in XTF. You can change these exports with a plugin, but I found it easier to both make some changes in our XTF XSLT files, and write a quick &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exportConverter.py&lt;/code&gt; script with lxml.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exportPublicData.py&lt;/code&gt; loops though all the resources modified since the last export and only exports records that are published. Since we still have a bunch of resources that only have really simple collection-level descriptions, I didn‚Äôt want to export EADs and PDFs for these, so I just exported the important information to a pipe delimited CSV.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I has always envisioned the ASpace API as completely replacing our spreadsheet that generates the static browse pages for our public access system. Yet, I didn‚Äôt want to 
export all that data over and over again, only make incremental updates like the EAD exports. Since my original static pages scripts were designed to loop though a spreadsheet, shifting to CSVs wasn‚Äôt very difficult. I also didn‚Äôt want to spend a lot of time changing things since we have some longer-term plans to export this data into a web framework.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exportPublicData.py&lt;/code&gt; updates this CSV data for modified collections as it exports EAD XML files. It then uses Python‚Äôs &lt;a href=&quot;https://docs.python.org/3/library/subprocess.html&quot;&gt;subprocess&lt;/a&gt; module to call git commands and version these exports and pushes them to &lt;a href=&quot;https://github.com/UAlbanyArchives/collections&quot;&gt;Github&lt;/a&gt;. It then copies new EAD files to our &lt;a href=&quot;http://meg.library.albany.edu:8080/archive/search?keyword=&amp;amp;browse-all=yes&quot;&gt;public XTF instance&lt;/a&gt;. Finally, it calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;staticPages.py&lt;/code&gt; to create all of the &lt;a href=&quot;http://library.albany.edu/speccoll/findaids/eresources/static/apap.html&quot;&gt;static&lt;/a&gt; &lt;a href=&quot;http://library.albany.edu/speccoll/findaids/eresources/static/alpha.html&quot;&gt;browse&lt;/a&gt; &lt;a href=&quot;http://library.albany.edu/speccoll/findaids/eresources/static/subjects.html&quot;&gt;pages&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I had some encoding errors that were a pain to fix, so I ended up writing these scripts for only Python 3+.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These scripts are set up to run nightly, a few hours before XTF re-indexes. The result is that staff can make changes in ASpace, create new collections or subjects, and the new public access system pages will appear overnight, without any manual processes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;timelines&quot;&gt;Timelines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;So, how long did this all take? We got ArchivesSpace up an running on a server near the end of December 2016, really started the migration scripts during the start of February, and finished up everything on the second week of April. Most of the migration prep time spent was in February and the first week of March, and most of March was devoted to staff testing/experimenting and writing the smaller workflow tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final migration ended up taking about 2 1/2 workdays, which I‚Äôm fairly happy with. That was the only downtime we had when changes couldn‚Äôt be made.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This was also not committing a large department-wide effort. It was mostly me managing everything, and although it was probably my primary project during that period, I was also managing the regular day to day tasks of the University Archives: doing accessions and reference, managing students, and working with other offices on campus to transfer their permanent records, and also service and (not very much) scholarship. I‚Äôm also lucky to have two excellent graduate students in the University Archives this year, Ben Covell and Erik Stolarski. It really can‚Äôt be understated how having really bright and hardworking students enables us to do so much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This timeframe is also a bit deceiving, as the real work started about 2 years ago when we worked to really standardize our descriptive data. A lot of the work we did back then really shortened up our timeline, as we didn‚Äôt have to do wholesale metadata cleanup before migrating. Our data still has some issues though, but ArchivesSpace has some features like the controlled values lists that can help fix some of our irregularities after migrating. The cool thing about this is that a lot of the clean up work can be distributed, as anyone in the department can easily make fixes whenever they see something.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, we cut some corners on the data-side ‚Äì particularly the locations. There are some long-term costs here as I decided to take lot of the labor required to straighten out all this data, and spread it out over time. I‚Äôm happy with this decision, as we have other competing priorities, like getting good collection-level records for everything, and overall increasing access to collections. We also could have put off the migration until some of our problems could be fixed, but there were significant maintenance costs to our old way of doing things that we really don‚Äôt have anymore. I‚Äôm hoping this frees us up to work on some of the more ambitious project we have planned.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;

&lt;p&gt;Greg Wiedeman&lt;/p&gt;</content><author><name>Gregory Wiedeman</name></author><summary type="html">This is an overview our ArchivesSpace migration process and descriptions of the code used. Hopefully this rather long-winded description helps document our processes and thinking.</summary></entry><entry><title type="html">Metadata (Book Review)</title><link href="http://localhost:4000/2017/03/01/metadata.html" rel="alternate" type="text/html" title="Metadata (Book Review)" /><published>2017-03-01T00:00:00-05:00</published><updated>2017-03-01T00:00:00-05:00</updated><id>http://localhost:4000/2017/03/01/metadata</id><content type="html" xml:base="http://localhost:4000/2017/03/01/metadata.html"></content><author><name>Gregory Wiedeman</name></author><summary type="html"></summary></entry></feed>